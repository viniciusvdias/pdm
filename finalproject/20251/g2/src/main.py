"""
Aplica√ß√£o principal para an√°lise de dados do RU-UFLA
"""

import click
import sys
import os
from pathlib import Path
from typing import Optional

# Adicionar src ao path para imports
sys.path.append(str(Path(__file__).parent))

from config import SparkConfig, DataPaths
from logging_config import setup_logging, get_module_logger
from data_analysis import RUAnalyzer

@click.group()
@click.option(
    "--log-level",
    type=click.Choice(["DEBUG", "INFO", "WARNING", "ERROR"]),
    default="INFO",
    help="N√≠vel de logging",
)
@click.option(
    "--log-to-file/--no-log-to-file",
    default=True,
    help="Se deve salvar logs em arquivo",
)
@click.option(
    "--log-colors/--no-log-colors", default=True, help="Se deve usar cores nos logs"
)
@click.pass_context
def cli(ctx, log_level, log_to_file, log_colors):
    """RU-UFLA Analytics: An√°lise de dados do Restaurante Universit√°rio da UFLA usando PySpark"""

    # Garantir que temos os diret√≥rios necess√°rios
    DataPaths.ensure_directories()

    # Configurar logging
    setup_logging(
        log_level=log_level, log_to_file=log_to_file, enable_colors=log_colors
    )

    # Contexto compartilhado
    ctx.ensure_object(dict)
    ctx.obj["log_level"] = log_level


@cli.command()
@click.option(
    "--master-url",
    help="URL do Spark master (ex: spark://spark-master:7077). Se n√£o especificado, usa configura√ß√£o padr√£o",
)
@click.option("--app-name", default="RU-UFLA-Analytics", help="Nome da aplica√ß√£o Spark")
@click.option(
    "--mode",
    type=click.Choice(["sample", "complete"]),
    default="complete",
    help="Modo de an√°lise: 'sample' para dados de amostra ou 'complete' para dataset completo",
)
@click.option(
    "--periods",
    help="Lista de per√≠odos letivos separados por v√≠rgula (formato: YYYY/S). Ex: 2024/1,2024/2",
)
@click.pass_context
def analyze(ctx, master_url: Optional[str], app_name: str, mode: str, periods: Optional[str]):
    """Executa an√°lise dos dados do RU-UFLA"""

    logger = get_module_logger("main")
    
    # Processar lista de per√≠odos
    periods_list = None
    if periods:
        import re
        
        # Separar per√≠odos por v√≠rgula e limpar espa√ßos
        periods_list = [p.strip() for p in periods.split(',') if p.strip()]
        
        # Padr√£o para validar formato YYYY/S
        period_pattern = r'^\d{4}/[12]$'
        
        # Validar cada per√≠odo
        for period in periods_list:
            if not re.match(period_pattern, period):
                raise click.BadParameter(f"Per√≠odo inv√°lido: {period}. Use formato YYYY/S (ex: 2024/1)")
        
        logger.info(f"Per√≠odos definidos: {', '.join(periods_list)}")
    else:
        logger.info("Nenhum per√≠odo espec√≠fico. Processando todos os dados.")
    
    if mode == "sample":
        logger.info("Iniciando an√°lise com dados de AMOSTRA do RU-UFLA")
        data_file = DataPaths.RU_DATA_SAMPLE
    else:
        logger.info("Iniciando an√°lise COMPLETA dos dados do RU-UFLA")
        data_file = DataPaths.RU_DATA_COMPLETE

    try:
        # Configurar Spark baseado no ambiente
        if master_url:
            os.environ["SPARK_MASTER_URL"] = master_url
            logger.info(f"Usando Spark master: {master_url}")

        # Criar sess√£o Spark
        spark = SparkConfig.get_spark_session(app_name)

        # Verificar se precisa fazer download para an√°lise completa
        if mode == "complete" and not os.path.exists(data_file):
            logger.info("Dataset completo n√£o encontrado. Fazendo download autom√°tico...")
            analyzer = RUAnalyzer(spark)
            analyzer.download_complete_dataset()
            
            # Verificar se o download foi bem-sucedido
            if not os.path.exists(data_file):
                raise FileNotFoundError(f"Falha no download. Dataset n√£o encontrado em: {data_file}")

        # Executar an√°lise
        analyzer = RUAnalyzer(spark)
        analyzer.run_complete_analysis(data_file, periods=periods_list)

        logger.success(f"An√°lise ({mode}) conclu√≠da com sucesso!")

    except Exception as e:
        logger.error(f"Erro durante a an√°lise: {e}")
        raise click.ClickException(f"Falha na an√°lise: {e}")
    finally:
        if "spark" in locals():
            spark.stop()

@cli.command()
@click.option("--master-url", help="URL do Spark master para teste de conectividade")
@click.option(
    "--app-name",
    default="RU-UFLA-Test-Connection",
    help="Nome da aplica√ß√£o Spark para teste",
)
@click.pass_context
def test_spark(ctx, master_url: Optional[str], app_name: str):
    """Testa a conectividade com o cluster Spark"""

    logger = get_module_logger("test")
    logger.info("Testando conectividade com Spark")

    try:
        if master_url:
            os.environ["SPARK_MASTER_URL"] = master_url
            logger.info(f"Testando conex√£o com: {master_url}")

        spark = SparkConfig.get_spark_session(app_name)

        # Teste simples
        test_data = spark.range(1000, numPartitions=4)
        count = test_data.count()

        logger.success(f"‚úÖ Conex√£o com Spark OK - Processou {count:,} registros")
        logger.info(f"Vers√£o do Spark: {spark.version}")
        logger.info(f"Master URL: {spark.conf.get('spark.master')}")
        logger.info(f"Paralelismo: {spark.sparkContext.defaultParallelism} cores")

    except Exception as e:
        logger.error(f"‚ùå Erro de conectividade: {e}")
        raise click.ClickException(f"Falha na conex√£o: {e}")
    finally:
        if "spark" in locals():
            spark.stop()

@cli.command()
@click.option(
    "--format",
    type=click.Choice(["table", "json"]),
    default="table",
    help="Formato de sa√≠da das informa√ß√µes",
)
def info(format):
    """Mostra informa√ß√µes sobre o ambiente e configura√ß√µes"""

    import platform
    import pyspark
    from datetime import datetime

    info_data = {
        "timestamp": datetime.now().isoformat(),
        "python_version": platform.python_version(),
        "pyspark_version": pyspark.__version__,
        "platform": platform.platform(),
        "spark_home": os.environ.get("SPARK_HOME", "N√£o definido"),
        "java_home": os.environ.get("JAVA_HOME", "N√£o definido"),
        "data_paths": {
            "base_dir": DataPaths.BASE_DIR,
            "data_sample": DataPaths.RU_DATA_SAMPLE,
            "results_dir": DataPaths.RESULTS_DIR,
            "metrics_dir": DataPaths.METRICS_DIR,
            "logs_dir": DataPaths.LOGS_DIR,
        },
    }

    if format == "json":
        import json

        click.echo(json.dumps(info_data, indent=2, ensure_ascii=False))
    else:
        click.echo("üîß RU-UFLA Analytics - Informa√ß√µes do Sistema")
        click.echo("=" * 50)
        click.echo(f"Timestamp: {info_data['timestamp']}")
        click.echo(f"Python: {info_data['python_version']}")
        click.echo(f"PySpark: {info_data['pyspark_version']}")
        click.echo(f"Plataforma: {info_data['platform']}")
        click.echo(f"SPARK_HOME: {info_data['spark_home']}")
        click.echo(f"JAVA_HOME: {info_data['java_home']}")
        click.echo()
        click.echo("üìÅ Caminhos de Dados:")
        for key, value in info_data["data_paths"].items():
            click.echo(f"  {key}: {value}")


@cli.command()
@click.option(
    "--master-url",
    help="URL do Spark master (ex: spark://spark-master:7077). Se n√£o especificado, usa configura√ß√£o padr√£o",
)
@click.option("--app-name", default="RU-UFLA-Experiments", help="Nome da aplica√ß√£o Spark")
@click.option(
    "--mode",
    type=click.Choice(["sample", "complete"]),
    default="complete",
    help="Modo de experimento: 'sample' para dados de amostra ou 'complete' para dataset completo",
)
@click.option(
    "--periods",
    help="Lista de per√≠odos letivos separados por v√≠rgula (formato: YYYY/S). Ex: 2024/1,2024/2",
)
@click.option(
    "--iterations",
    type=int,
    default=3,
    help="N√∫mero de itera√ß√µes por configura√ß√£o de experimento",
)
@click.pass_context
def experiments(ctx, master_url: Optional[str], app_name: str, mode: str, periods: Optional[str], iterations: int):
    """Executa suite completa de experimentos isolados com m√∫ltiplas configura√ß√µes"""

    logger = get_module_logger("experiments")
    
    # Processar lista de per√≠odos
    periods_list = None
    if periods:
        import re
        
        # Separar per√≠odos por v√≠rgula e limpar espa√ßos
        periods_list = [p.strip() for p in periods.split(',') if p.strip()]
        
        # Padr√£o para validar formato YYYY/S
        period_pattern = r'^\d{4}/[12]$'
        
        # Validar cada per√≠odo
        for period in periods_list:
            if not re.match(period_pattern, period):
                raise click.BadParameter(f"Per√≠odo inv√°lido: {period}. Use formato YYYY/S (ex: 2024/1)")
        
        logger.info(f"Per√≠odos definidos: {', '.join(periods_list)}")
    else:
        logger.info("Nenhum per√≠odo espec√≠fico. Processando todos os dados.")
    
    if mode == "sample":
        logger.info(f"Iniciando suite de experimentos com dados de AMOSTRA ({iterations} itera√ß√µes)")
        data_file = DataPaths.RU_DATA_SAMPLE
    else:
        logger.info(f"Iniciando suite de experimentos com dados COMPLETOS ({iterations} itera√ß√µes)")
        data_file = DataPaths.RU_DATA_COMPLETE

    try:
        # Configurar Spark baseado no ambiente
        if master_url:
            os.environ["SPARK_MASTER_URL"] = master_url
            logger.info(f"Usando Spark master: {master_url}")

        # Criar sess√£o Spark
        spark = SparkConfig.get_spark_session(app_name)

        # Verificar se precisa fazer download para an√°lise completa
        if mode == "complete" and not os.path.exists(data_file):
            logger.info("Dataset completo n√£o encontrado. Fazendo download autom√°tico...")
            analyzer = RUAnalyzer(spark)
            analyzer.download_complete_dataset()
            
            # Verificar se o download foi bem-sucedido
            if not os.path.exists(data_file):
                raise FileNotFoundError(f"Falha no download. Dataset n√£o encontrado em: {data_file}")

        # Executar experimentos
        analyzer = RUAnalyzer(spark)
        
        # Carregar dados primeiro
        analyzer.load_data(data_file)
        if periods_list:
            analyzer.apply_period_filter(periods_list)
        
        # Executar suite de experimentos usando o m√≥dulo experiments
        from experiments import run_experiment_suite
        run_experiment_suite(spark, analyzer.df, mode, iterations, logger)

        logger.success(f"Suite de experimentos ({iterations} itera√ß√µes) conclu√≠da com sucesso!")

    except Exception as e:
        logger.error(f"Erro durante os experimentos: {e}")
        raise click.ClickException(f"Falha nos experimentos: {e}")
    finally:
        if "spark" in locals():
            spark.stop()

@cli.command()
@click.option(
    "--pattern",
    default="experiment_*",
    help="Padr√£o para buscar arquivos de resultados (ex: experiment_complete_*)",
)
@click.option(
    "--output-format",
    type=click.Choice(["json", "csv", "both"]),
    default="both",
    help="Formato de sa√≠da da consolida√ß√£o",
)
@click.pass_context
def consolidate(ctx, pattern: str, output_format: str):
    """Consolida m√∫ltiplos resultados de experimentos em um relat√≥rio √∫nico"""

    logger = get_module_logger("consolidate")
    
    import glob
    import json
    import csv
    from datetime import datetime
    
    logger.info(f"Consolidando resultados com padr√£o: {pattern}")
    
    # Buscar arquivos de resultados
    search_path = f"{DataPaths.RESULTS_DIR}/{pattern}.json"
    result_files = glob.glob(search_path)
    
    if not result_files:
        logger.warning(f"Nenhum arquivo encontrado com padr√£o: {search_path}")
        return
    
    logger.info(f"Encontrados {len(result_files)} arquivos de resultados")
    
    # Consolidar resultados
    consolidated_data = {
        "timestamp": datetime.now().isoformat(),
        "total_files": len(result_files),
        "pattern": pattern,
        "experiments": []
    }
    
    all_workloads = []
    
    for file_path in sorted(result_files):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            consolidated_data["experiments"].append({
                "file": file_path.split('/')[-1],
                "summary": data.get("summary", {}),
                "config": data.get("spark_config", {}),
                "workloads_count": len(data.get("workloads", []))
            })
            
            # Adicionar workloads individuais para an√°lise
            for workload in data.get("workloads", []):
                workload_entry = {
                    "file": file_path.split('/')[-1],
                    "iteration": data.get("iteration", 0),
                    "dataset": data.get("dataset_size", "unknown"),
                    "config_parallelism": data.get("spark_config", {}).get("parallelism", 1),
                    "config_partitions": data.get("spark_config", {}).get("partitions", 1),
                    **workload
                }
                all_workloads.append(workload_entry)
                
            logger.info(f"Processado: {file_path.split('/')[-1]}")
            
        except Exception as e:
            logger.warning(f"Erro ao processar {file_path}: {e}")
    
    # Salvar consolida√ß√£o
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if output_format in ["json", "both"]:
        json_file = f"{DataPaths.RESULTS_DIR}/consolidated_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(consolidated_data, f, indent=2, ensure_ascii=False)
        logger.success(f"Consolida√ß√£o JSON salva: {json_file}")
    
    if output_format in ["csv", "both"]:
        csv_file = f"{DataPaths.RESULTS_DIR}/consolidated_workloads_{timestamp}.csv"
        
        if all_workloads:
            # Extrair todas as chaves poss√≠veis
            all_keys = set()
            for workload in all_workloads:
                all_keys.update(workload.keys())
                if 'metricas' in workload:
                    for metric_key in workload['metricas'].keys():
                        all_keys.add(f"metrics_{metric_key}")
            
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # Cabe√ßalho
                header = [
                    'File', 'Iteration', 'Dataset', 'Config_Parallelism', 'Config_Partitions',
                    'Workload', 'Nome', 'Execution_Time_s', 'Throughput', 'Memory_GB',
                    'Stages', 'Tasks', 'CPU_ms', 'GC_ms', 'Shuffle_Read_Bytes', 'Shuffle_Write_Bytes'
                ]
                writer.writerow(header)
                
                # Dados
                for workload in all_workloads:
                    metrics = workload.get('metricas', {})
                    writer.writerow([
                        workload.get('file', ''),
                        workload.get('iteration', 0),
                        workload.get('dataset', ''),
                        workload.get('config_parallelism', 1),
                        workload.get('config_partitions', 1),
                        workload.get('workload', ''),
                        workload.get('nome', ''),
                        metrics.get('execution_time_seconds', 0),
                        metrics.get('throughput_per_second', 0),
                        metrics.get('memory_peak_gb', 0),
                        metrics.get('stages_count', 0),
                        metrics.get('tasks_count', 0),
                        metrics.get('cpu_time_ms', 0),
                        metrics.get('gc_time_ms', 0),
                        metrics.get('shuffle_read_bytes', 0),
                        metrics.get('shuffle_write_bytes', 0)
                    ])
        
        logger.success(f"Consolida√ß√£o CSV salva: {csv_file}")
    
    # Estat√≠sticas resumidas
    logger.info("=== ESTAT√çSTICAS CONSOLIDADAS ===")
    logger.info(f"Total de arquivos processados: {len(result_files)}")
    logger.info(f"Total de workloads: {len(all_workloads)}")
    
    if all_workloads:
        # Estat√≠sticas por workload
        workload_stats = {}
        for workload in all_workloads:
            wl_name = workload.get('workload', 'unknown')
            if wl_name not in workload_stats:
                workload_stats[wl_name] = []
            workload_stats[wl_name].append(workload.get('metricas', {}).get('execution_time_seconds', 0))
        
        for wl_name, times in workload_stats.items():
            avg_time = sum(times) / len(times) if times else 0
            logger.info(f"{wl_name}: {len(times)} execu√ß√µes, tempo m√©dio: {avg_time:.2f}s")
    
    logger.success("Consolida√ß√£o conclu√≠da com sucesso!")


if __name__ == "__main__":
    cli()
