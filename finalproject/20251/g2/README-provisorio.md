# ğŸ“ RU-UFLA Analytics - Setup Inicial

> **Status:** Estrutura inicial configurada âœ…  
> **PrÃ³ximos passos:** ImplementaÃ§Ã£o das anÃ¡lises especÃ­ficas

## ğŸ“‹ Sobre o Projeto

AnÃ¡lise de dados do Restaurante UniversitÃ¡rio (RU) da Universidade Federal de Lavras (UFLA) utilizando processamento distribuÃ­do com Apache Spark.

### ğŸ¯ Objetivos

- **AnÃ¡lise de SÃ©ries Temporais:** DetecÃ§Ã£o de picos e tendÃªncias de consumo
- **AnÃ¡lise de Grafos:** Co-ocorrÃªncia e detecÃ§Ã£o de grupos sociais
- **MÃ©tricas de Performance:** Monitoramento detalhado com sparkMeasure

### ğŸ—ï¸ Arquitetura

- **Cluster Docker Swarm** com Apache Spark
- **Driver + 2 Workers** para processamento distribuÃ­do
- **Jupyter Notebooks** para desenvolvimento interativo
- **sparkMeasure** para coleta de mÃ©tricas de performance

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Docker
- Docker Compose

### Comandos Principais

```bash
# Tornar script executÃ¡vel
chmod +x bin/run_project.sh

# Construir e iniciar o cluster (primeiro uso)
./bin/run_project.sh build
./bin/run_project.sh up

# Ou simplesmente (jÃ¡ constrÃ³i automaticamente)
./bin/run_project.sh
```

### ğŸŒ Interfaces Web

ApÃ³s inicializar o cluster:

| ServiÃ§o                  | URL                   | DescriÃ§Ã£o                   |
| ------------------------ | --------------------- | --------------------------- |
| **Jupyter Notebook**     | http://localhost:8888 | Desenvolvimento interativo  |
| **Spark Master UI**      | http://localhost:8080 | Monitoramento do cluster    |
| **Spark Application UI** | http://localhost:4040 | MÃ©tricas da aplicaÃ§Ã£o atual |
| **Worker 1 UI**          | http://localhost:8081 | Status do Worker 1          |
| **Worker 2 UI**          | http://localhost:8082 | Status do Worker 2          |

### ğŸ“ Estrutura do Projeto

```
g2/
â”œâ”€â”€ bin/                    # Scripts executÃ¡veis
â”‚   â””â”€â”€ run_project.sh     # Script principal
â”œâ”€â”€ src/                    # CÃ³digo fonte Python
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # ConfiguraÃ§Ãµes Spark
â”‚   â””â”€â”€ spark_measure_utils.py  # UtilitÃ¡rios sparkMeasure
â”œâ”€â”€ notebooks/              # Jupyter Notebooks
â”‚   â””â”€â”€ 01_exploracao_inicial.ipynb
â”œâ”€â”€ misc/                   # Arquivos auxiliares
â”‚   â”œâ”€â”€ results/           # Resultados das anÃ¡lises
â”‚   â””â”€â”€ metrics/           # MÃ©tricas de performance
â”œâ”€â”€ datasample/            # Dados de amostra (< 1MB)
â”‚   â””â”€â”€ ru_sample.csv
â”œâ”€â”€ presentation/          # Slides da apresentaÃ§Ã£o
â”œâ”€â”€ docker-compose.yml     # ConfiguraÃ§Ã£o do cluster
â”œâ”€â”€ Dockerfile            # Imagem Docker personalizada
â”œâ”€â”€ pyproject.toml        # DependÃªncias Python (UV)
â””â”€â”€ README-provisorio.md  # Esta documentaÃ§Ã£o
```

## ğŸ› ï¸ Stack TecnolÃ³gica

- **ğŸ Python 3.12** com UV para gerenciamento de dependÃªncias
- **âš¡ Apache Spark 3.5.0** para processamento distribuÃ­do
- **ğŸ“Š sparkMeasure 0.25** - API oficial para mÃ©tricas de performance
- **ğŸ“ loguru** - Sistema de logging estruturado e colorido
- **ğŸ³ Docker & Docker Compose** para containerizaÃ§Ã£o
- **ğŸ““ Jupyter Notebook** para anÃ¡lise interativa
- **ğŸ”¥ PySpark** para interface Python do Spark

## ğŸ“Š Dados

### Fonte

- **Dataset:** Registros pÃºblicos do RU-UFLA (2009-presente)
- **Volume:** MilhÃµes de registros estruturados
- **Atributos:** cidade de nascimento, data de consumo, curso, etc.

### Amostra DisponÃ­vel

- Arquivo: `datasample/ru_sample.csv`
- Tamanho: < 1MB (para testes rÃ¡pidos)

## ğŸ¯ PrÃ³ximas ImplementaÃ§Ãµes

### AnÃ¡lises Planejadas

1. **SÃ©ries Temporais**

   - TendÃªncias de consumo por perÃ­odo
   - DetecÃ§Ã£o de sazonalidade
   - Picos de demanda

2. **AnÃ¡lise de Grafos**

   - Redes de co-ocorrÃªncia entre estudantes
   - DetecÃ§Ã£o de comunidades
   - MÃ©tricas de centralidade

3. **VisualizaÃ§Ãµes**
   - Dashboards interativos
   - RelatÃ³rios automÃ¡ticos
   - GrÃ¡ficos de performance

### MÃ©tricas de Performance

- Tempo de execuÃ§Ã£o por estÃ¡gio
- Uso de CPU, memÃ³ria e I/O
- MÃ©tricas de shuffle e garbage collection
- Armazenamento em CSV/JSON para anÃ¡lise posterior

## ğŸ”§ Comandos Ãšteis

```bash
# Ver status do cluster
./bin/run_project.sh status

# Ver logs em tempo real
./bin/run_project.sh logs

# Reiniciar cluster
./bin/run_project.sh restart

# Parar cluster
./bin/run_project.sh down

# Limpeza completa
./bin/run_project.sh clean

# Abrir interfaces no navegador
./bin/run_project.sh jupyter
./bin/run_project.sh spark-ui

# Ver ajuda
./bin/run_project.sh help
```

## ğŸ“ Desenvolvimento

### Workflow Recomendado

1. **Desenvolvimento:** Use Jupyter Notebook (http://localhost:8888)
2. **CÃ³digo modular:** Organize funÃ§Ãµes em `src/`
3. **Testes:** Execute no cluster distribuÃ­do
4. **MÃ©tricas:** Use sparkMeasure para performance
5. **Resultados:** Salve em `misc/results/`

### Boas PrÃ¡ticas

- Sempre use o `SparkConfig.get_spark_session()` para sessÃµes Spark
- Use `@measure_spark_operation` para decorar funÃ§Ãµes importantes
- Para anÃ¡lises rÃ¡pidas, use `QuickMeasure.time_spark_sql()`
- Use `SparkMeasureWrapper` para controle fino das mediÃ§Ãµes
- Salve mÃ©tricas regularmente para anÃ¡lise de performance
- Mantenha dados grandes fora do repositÃ³rio

### ğŸ§ª Testando a InstalaÃ§Ã£o

```bash
# Testar integraÃ§Ã£o completa
./bin/run_project.sh test

# Ver logs em tempo real
./bin/run_project.sh logs

# Demonstrar sistema de logging
docker exec -it spark-master python /app/src/example_logging.py
```

### ğŸ“ Sistema de Logging

O projeto usa **loguru** para logging estruturado e profissional:

#### **ConfiguraÃ§Ã£o via VariÃ¡veis de Ambiente:**

```bash
# Configurar nÃ­vel de log
export LOG_LEVEL=DEBUG    # DEBUG, INFO, WARNING, ERROR

# Habilitar/desabilitar arquivo de log
export LOG_TO_FILE=true   # true, false

# Habilitar/desabilitar cores no terminal
export LOG_COLORS=true    # true, false

# Executar com configuraÃ§Ã£o personalizada
LOG_LEVEL=DEBUG ./bin/run_project.sh up
```

#### **Arquivos de Log Gerados:**

- **`misc/logs/ru_ufla_analytics.log`** - Log geral com rotaÃ§Ã£o (10MB)
- **`misc/logs/errors.log`** - Apenas erros (5MB)
- **`misc/logs/performance.log`** - MÃ©tricas de performance (20MB)

#### **Recursos do Logging:**

- âœ… **Logs coloridos** no terminal para melhor visualizaÃ§Ã£o
- âœ… **RotaÃ§Ã£o automÃ¡tica** de arquivos (evita arquivos gigantes)
- âœ… **CompressÃ£o** automÃ¡tica dos logs antigos
- âœ… **Thread-safe** para ambientes multi-threaded
- âœ… **Structured logging** com contexto de mÃ³dulos
- âœ… **Stack traces** detalhados em caso de erro
- âœ… **InterceptaÃ§Ã£o** de logs do Python padrÃ£o e PySpark

#### **Exemplo de Uso no CÃ³digo:**

```python
from loguru import logger
from logging_config import get_module_logger

# Logger bÃ¡sico
logger.info("Mensagem informativa")
logger.warning("Aviso importante")
logger.error("Erro ocorreu")

# Logger especÃ­fico para mÃ³dulo
data_logger = get_module_logger("data_processing")
data_logger.success("Dados carregados: {} registros", count)

# Logger de performance
perf_logger = get_performance_logger()
perf_logger.info("Tempo de execuÃ§Ã£o: {}s", execution_time)
```

---

**ğŸ“§ Contato:** Grupo 2 - Big Data  
**ğŸ“… VersÃ£o:** 0.1.0 (Setup Inicial)  
**ğŸ”„ Ãšltima atualizaÃ§Ã£o:** Janeiro 2025
