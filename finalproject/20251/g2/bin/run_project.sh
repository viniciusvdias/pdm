#!/bin/bash

# Script principal para executar o projeto RU-UFLA Analytics
# Suporte para execu√ß√£o local, Docker Compose e Docker Swarm
# Autor: Grupo 2

set -e

echo "üöÄ Iniciando projeto RU-UFLA Analytics..."

# Verificar se Docker est√° instalado
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker n√£o encontrado. Por favor, instale o Docker primeiro."
    exit 1
fi

# Detectar qual vers√£o do Docker Compose est√° dispon√≠vel
DOCKER_COMPOSE_CMD=""
if command -v docker-compose &> /dev/null; then
    DOCKER_COMPOSE_CMD="docker-compose"
elif docker compose version &> /dev/null; then
    DOCKER_COMPOSE_CMD="docker compose"
else
    echo "‚ùå Docker Compose n√£o encontrado. Por favor, instale o Docker Compose primeiro."
    echo "   Suporte para: 'docker-compose' (V1) ou 'docker compose' (V2)"
    exit 1
fi

echo "üîß Usando: $DOCKER_COMPOSE_CMD"

# Configurar vari√°veis de ambiente para logging
export LOG_LEVEL=${LOG_LEVEL:-INFO}
export LOG_TO_FILE=${LOG_TO_FILE:-true}
export LOG_COLORS=${LOG_COLORS:-true}

# Fun√ß√£o para mostrar ajuda
show_help() {
    echo "üìñ Uso: $0 [COMANDO]"
    echo ""
    echo "Comandos dispon√≠veis:"
    echo "  build           - Construir as imagens Docker"
    echo "  up              - Subir o cluster Spark local (padr√£o)"
    echo "  down            - Parar o cluster Spark local"
    echo "  restart         - Reiniciar o cluster Spark local"
    echo "  logs            - Mostrar logs dos containers"
    echo "  analyze [mode] [periods]  - Executar an√°lise de dados (mode: sample|complete)"
    echo "  experiments [mode] [iterations] [periods] - Executar experimentos isolados (mode: sample|complete, iterations: n√∫mero >= 1)"
    echo "  consolidate [pattern] - Consolidar resultados de experimentos"
    echo ""
    echo "üê≥ Comandos Docker Swarm:"
    echo "  swarm-init      - Inicializar Docker Swarm"
    echo "  swarm-deploy    - Deploy no Docker Swarm (4 n√≥s)"
    echo "  swarm-scale     - Escalar servi√ßos no Swarm"
    echo "  swarm-remove    - Remover stack do Swarm"
    echo "  swarm-logs      - Logs dos servi√ßos no Swarm"
    echo "  swarm-status    - Status dos servi√ßos no Swarm"
    echo "  swarm-analyze   - Executar an√°lise no cluster Swarm"
    echo ""
    echo "  help            - Mostrar esta ajuda"
    echo ""
    echo "üåê URLs importantes:"
    echo "  Spark Master UI:  http://localhost:8080"
    echo "  Spark App UI:     http://localhost:4040"
    echo "  Worker 1 UI:      http://localhost:8081"
    echo "  Worker 2 UI:      http://localhost:8082"
    echo ""
    echo "üîß Vari√°veis de ambiente para logging:"
    echo "  LOG_LEVEL=DEBUG|INFO|WARNING|ERROR (padr√£o: INFO)"
    echo "  LOG_TO_FILE=true|false (padr√£o: true)"
    echo "  LOG_COLORS=true|false (padr√£o: true)"
    echo ""
    echo "üí° Exemplos de uso:"
    echo "  LOG_LEVEL=DEBUG $0 up"
    echo "  $0 analyze sample                      # An√°lise com dados de amostra"
    echo "  $0 analyze complete                    # An√°lise completa (padr√£o)"
    echo "  $0 analyze sample 2024/1,2024/2          # An√°lise do ano 2024 com dados de amostra"
    echo "  $0 analyze complete 2024/1,2024/2        # An√°lise do ano 2024 com dados completos"
    echo "  $0 experiments sample 5                # Experimentos isolados com 5 itera√ß√µes (dados de amostra)"
    echo "  $0 experiments complete 3 2024/1,2024/2 # Experimentos completos com 3 itera√ß√µes (ano 2024)"
    echo "  $0 consolidate experiment_complete_*   # Consolidar resultados de experimentos completos"
}

# Fun√ß√£o para construir imagens
build_images() {
    echo "üî® Construindo imagens Docker..."
    $DOCKER_COMPOSE_CMD build
    echo "‚úÖ Imagens constru√≠das com sucesso!"
}

# Fun√ß√£o para subir o cluster local
start_cluster() {
    echo "üöÄ Iniciando cluster Spark local..."
    $DOCKER_COMPOSE_CMD up -d spark-master spark-worker-1 spark-worker-2
    
    echo "‚è≥ Aguardando inicializa√ß√£o dos servi√ßos..."
    sleep 15
    
    echo "‚úÖ Cluster iniciado com sucesso!"
    echo ""
    echo "üåê Servi√ßos dispon√≠veis:"
    echo "  üî• Spark Master UI:  http://localhost:8080"
    echo "  üìà Spark App UI:     http://localhost:4040 (quando app estiver rodando)"
    echo "  üë∑ Worker 1 UI:      http://localhost:8081"
    echo "  üë∑ Worker 2 UI:      http://localhost:8082"
    echo ""
    echo "üìã Comandos √∫teis:"
    echo "  Logs:       $0 logs"
    echo "  An√°lise:    $0 analyze [sample|complete]"
    echo "  Parar:      $0 down"
}

# Fun√ß√£o para parar o cluster
stop_cluster() {
    echo "üõë Parando cluster Spark..."
    $DOCKER_COMPOSE_CMD down
    echo "‚úÖ Cluster parado com sucesso!"
}

# Fun√ß√£o para reiniciar o cluster
restart_cluster() {
    echo "üîÑ Reiniciando cluster Spark..."
    $DOCKER_COMPOSE_CMD restart
    echo "‚úÖ Cluster reiniciado com sucesso!"
}

# Fun√ß√£o para mostrar logs
show_logs() {
    echo "üìã Mostrando logs dos containers..."
    $DOCKER_COMPOSE_CMD logs -f
}

# Fun√ß√£o para executar an√°lise
run_analysis() {
    local mode=${1:-complete}
    local periods=$2
 
    case "$mode" in
        sample)
            echo "üìä Executando an√°lise de dados do RU-UFLA (AMOSTRA)..."
            ;;
        complete)
            echo "üìä Executando an√°lise de dados do RU-UFLA (COMPLETA)..."
            ;;
        *)
            echo "‚ùå Modo inv√°lido: $mode. Use 'sample' ou 'complete'"
            return 1
            ;;
    esac
    
    # Verificar se o cluster est√° rodando
    if ! $DOCKER_COMPOSE_CMD ps | grep -q "Up"; then
        echo "‚ö†Ô∏è  Cluster n√£o est√° rodando. Iniciando..."
        start_cluster
        sleep 20
    fi
    
    # Executar an√°lise com o modo especificado
    if [ -z "$periods" ]; then
        $DOCKER_COMPOSE_CMD run --rm analytics /app/.venv/bin/python -m src.main analyze --master-url spark://spark-master:7077 --mode $mode
    else
        $DOCKER_COMPOSE_CMD run --rm analytics /app/.venv/bin/python -m src.main analyze --master-url spark://spark-master:7077 --mode $mode --periods $periods
    fi
    echo "‚úÖ An√°lise conclu√≠da!"
}

# Fun√ß√£o para executar experimentos isolados
run_experiments() {
    local mode=${1:-complete}
    local iterations=${2:-3}
    local periods=$3
 
    case "$mode" in
        sample)
            echo "üß™ Executando experimentos isolados do RU-UFLA (AMOSTRA)..."
            ;;
        complete)
            echo "üß™ Executando experimentos isolados do RU-UFLA (COMPLETA)..."
            ;;
        *)
            echo "‚ùå Modo inv√°lido: $mode. Use 'sample' ou 'complete'"
            return 1
            ;;
    esac
    
    # Validar n√∫mero de itera√ß√µes
    if ! [[ "$iterations" =~ ^[0-9]+$ ]] || [ "$iterations" -lt 1 ]; then
        echo "‚ùå N√∫mero de itera√ß√µes inv√°lido: $iterations. Use um n√∫mero inteiro >= 1"
        return 1
    fi
    
    echo "üîÑ Executando $iterations itera√ß√µes por configura√ß√£o"
    
    # Verificar se o cluster est√° rodando
    if ! $DOCKER_COMPOSE_CMD ps | grep -q "Up"; then
        echo "‚ö†Ô∏è  Cluster n√£o est√° rodando. Iniciando..."
        start_cluster
        sleep 20
    fi
    
    # Executar experimentos isolados com os par√¢metros especificados
    if [ -z "$periods" ]; then
        $DOCKER_COMPOSE_CMD run --rm analytics /app/.venv/bin/python -m src.main experiments --master-url spark://spark-master:7077 --mode $mode --iterations $iterations
    else
        $DOCKER_COMPOSE_CMD run --rm analytics /app/.venv/bin/python -m src.main experiments --master-url spark://spark-master:7077 --mode $mode --iterations $iterations --periods $periods
    fi
    echo "‚úÖ Experimentos isolados conclu√≠dos!"
}

# Fun√ß√£o para consolidar resultados de experimentos
consolidate_results() {
    local pattern=${1:-"experiment_*"}
    
    echo "üìä Consolidando resultados de experimentos..."
    echo "   Padr√£o de busca: $pattern"
    
    # Verificar se o cluster est√° rodando
    if ! $DOCKER_COMPOSE_CMD ps | grep -q "Up"; then
        echo "‚ö†Ô∏è  Cluster n√£o est√° rodando. Iniciando..."
        start_cluster
        sleep 20
    fi
    
    $DOCKER_COMPOSE_CMD run --rm analytics /app/.venv/bin/python -m src.main consolidate --pattern "$pattern"
    echo "‚úÖ Consolida√ß√£o conclu√≠da!"
}
# === FUN√á√ïES DOCKER SWARM ===

# Fun√ß√£o para inicializar Docker Swarm
init_swarm() {
    echo "üê≥ Inicializando Docker Swarm..."
    
    if docker info | grep -q "Swarm: active"; then
        echo "‚ÑπÔ∏è  Docker Swarm j√° est√° ativo"
    else
        docker swarm init
        echo "‚úÖ Docker Swarm inicializado!"
    fi
}

# Fun√ß√£o para deploy no Docker Swarm
deploy_swarm() {
    echo "üöÄ Fazendo deploy no Docker Swarm..."
    
    # Verificar se Swarm est√° ativo
    if ! docker info | grep -q "Swarm: active"; then
        echo "‚ö†Ô∏è  Docker Swarm n√£o est√° ativo. Inicializando..."
        init_swarm
    fi
    
    # Construir imagem
    docker build -t ru-ufla-analytics:latest .
    
    # Deploy da stack
    docker stack deploy -c docker-compose.swarm.yml ru-analytics
    
    echo "‚úÖ Deploy no Swarm conclu√≠do!"
    echo "üåê Acesse http://localhost:8080 para o Spark Master UI"
    
    # Mostrar status
    swarm_status
}

# Fun√ß√£o para escalar servi√ßos no Swarm
scale_swarm() {
    local workers=${1:-8}
    echo "üìà Escalando workers para $workers r√©plicas..."
    
    if ! docker stack services ru-analytics | grep -q "ru-analytics_spark-worker"; then
        echo "‚ùå Stack ru-analytics n√£o encontrada. Execute 'swarm-deploy' primeiro."
        return 1
    fi
    
    docker service scale ru-analytics_spark-worker=$workers
    echo "‚úÖ Scaling conclu√≠do!"
    
    # Mostrar status atualizado
    echo "üìä Status atualizado:"
    docker service ls --filter name=ru-analytics
}

# Fun√ß√£o para remover stack do Swarm
remove_swarm() {
    echo "üóëÔ∏è  Removendo stack do Docker Swarm..."
    docker stack rm ru-analytics
    echo "‚úÖ Stack removida!"
}

# Fun√ß√£o para logs do Swarm
swarm_logs() {
    echo "üìã Logs dos servi√ßos no Swarm:"
    docker service logs -f ru-analytics_spark-master
}

# Fun√ß√£o para status do Swarm
swarm_status() {
    echo "üìä Status dos servi√ßos no Docker Swarm:"
    docker stack services ru-analytics
    echo ""
    echo "üìù Tarefas (containers) por servi√ßo:"
    docker stack ps ru-analytics --no-trunc
    echo ""
    echo "üîç N√≥s do cluster:"
    docker node ls
}

# Fun√ß√£o para executar an√°lise no Swarm
run_swarm_analysis() {
    local mode=${1:-complete}
    local periods=$2
    
    case "$mode" in
        sample)
            echo "üìä Executando an√°lise de dados do RU-UFLA (AMOSTRA) no cluster Swarm..."
            ;;
        complete)
            echo "üìä Executando an√°lise de dados do RU-UFLA (COMPLETA) no cluster Swarm..."
            ;;
        *)
            echo "‚ùå Modo inv√°lido: $mode. Use 'sample' ou 'complete'"
            return 1
            ;;
    esac
    
    # Verificar se o cluster Swarm est√° rodando
    if ! docker stack services ru-analytics | grep -q "ru-analytics_spark-master"; then
        echo "‚ö†Ô∏è  Cluster Swarm n√£o est√° rodando. Fazendo deploy..."
        deploy_swarm
        sleep 60
    fi
    
    # Executar an√°lise com o modo especificado
    if [ -z "$periods" ]; then
        docker service create --name ru-analytics-job-$(date +%s) \
            --network ru-analytics_spark-network \
            --mount type=bind,source=$(pwd)/misc,destination=/app/misc \
            --mount type=bind,source=$(pwd)/datasample,destination=/app/datasample \
            --env SPARK_MASTER_URL=spark://spark-master:7077 \
            --env DOCKER_SWARM_MODE=true \
            --constraint 'node.role == manager' \
            --restart-condition none \
            ru-ufla-analytics:latest \
            /app/.venv/bin/python -m src.main analyze --master-url spark://spark-master:7077 --mode $mode
    else
        docker service create --name ru-analytics-job-$(date +%s) \
            --network ru-analytics_spark-network \
            --mount type=bind,source=$(pwd)/misc,destination=/app/misc \
            --mount type=bind,source=$(pwd)/datasample,destination=/app/datasample \
            --env SPARK_MASTER_URL=spark://spark-master:7077 \
            --env DOCKER_SWARM_MODE=true \
            --constraint 'node.role == manager' \
            --restart-condition none \
            ru-ufla-analytics:latest \
            /app/.venv/bin/python -m src.main analyze --master-url spark://spark-master:7077 --mode $mode --periods $periods
    fi
    
    echo "‚úÖ Job de an√°lise iniciado no cluster Swarm!"
    echo "üí° Use 'docker service logs -f <job-name>' para acompanhar o progresso"
}

# Processar comando
case "${1:-up}" in
    build)
        build_images
        ;;
    up|start)
        start_cluster
        ;;
    down|stop)
        stop_cluster
        ;;
    restart)
        restart_cluster
        ;;
    logs)
        show_logs
        ;;
    analyze)
        run_analysis $2 $3
        ;;
    experiments)
        run_experiments $2 $3 $4
        ;;
    consolidate)
        consolidate_results $2
        ;;
    # Comandos Docker Swarm
    swarm-init)
        init_swarm
        ;;
    swarm-deploy)
        deploy_swarm
        ;;
    swarm-scale)
        scale_swarm $2
        ;;
    swarm-remove)
        remove_swarm
        ;;
    swarm-logs)
        swarm_logs
        ;;
    swarm-status)
        swarm_status
        ;;
    swarm-analyze)
        run_swarm_analysis $2 $3
        ;;
    help)
        show_help
        ;;
    *)
        echo "‚ùå Comando desconhecido: $1"
        echo ""
        show_help
        exit 1
        ;;
esac 