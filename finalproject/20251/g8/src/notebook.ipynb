{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7ea63f-3e33-4bdb-982f-6f913d25d86d",
   "metadata": {},
   "source": [
    "### Carregamento e Integração dos Dados\n",
    "\n",
    "Inicialmente, o diretório `/home/jovyan/work` é adicionado ao caminho de busca do Python, viabilizando a importação de módulos utilitários desenvolvidos para o projeto. Em seguida, utiliza-se a classe `DataLoader`, do pacote `utils`, para realizar a leitura dos arquivos CSV referentes às ocorrências de acidentes e às pessoas envolvidas, armazenados em subdiretórios distintos.\n",
    "\n",
    "Os dados carregados são integrados por meio de uma junção (`join`) baseada na coluna `id`, resultando em um `DataFrame` consolidado (`df_joined`) que reúne informações tanto das ocorrências quanto dos envolvidos, servindo como base para as análises posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c559465f-a7e2-4540-896d-a0a51a2e6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/09 05:26:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"BigData\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e28786d-fc1c-4875-a56b-3830a114aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WORKLOAD-1] Carregamento dos dados de ocorrências (Carga Ocorrências)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRIC] Tempo de execução (Carga Ocorrências): 15.18 segundos\n",
      "[WORKLOAD-2] Carregamento dos dados de pessoas (Carga Pessoas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 05:26:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRIC] Tempo de execução (Carga Pessoas): 14.84 segundos\n",
      "[WORKLOAD-3] Junção entre ocorrências e pessoas por ID (Join Ocorrências + Pessoas)\n",
      "[METRIC] Tempo de execução (Join Ocorrências + Pessoas): 0.03 segundos\n",
      "Dados de ocorrências carregados:\n",
      "+-------+------------+-----------+--------+---+---+-----+-----------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----+-------+------+-------------+--------------+------+---------+-------+--------+\n",
      "|     id|data_inversa| dia_semana| horario| uf| br|   km|  municipio|      causa_acidente|       tipo_acidente|classificacao_acidente|   fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo| ano|pessoas|mortos|feridos_leves|feridos_graves|ilesos|ignorados|feridos|veiculos|\n",
      "+-------+------------+-----------+--------+---+---+-----+-----------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----+-------+------+-------------+--------------+------+---------+-------+--------+\n",
      "|1228858|  2013-01-01|terça-feira|01:00:00| PA|163|  992|   SANTAREM|Defeito mecânico ...|Queda de motocicl...|   Com Vítimas Feridas|Plena noite|  Crescente|               Nublado|   Simples|      Curva|   Rural|2013|      2|     0|            2|             0|     0|        0|      2|       1|\n",
      "|1228860|  2013-01-01|terça-feira|01:00:00| PR|476|  114|    COLOMBO|Velocidade incomp...|Colisão com objet...|           Sem Vítimas|Plena noite|Decrescente|               Nublado|   Simples|      Curva|  Urbano|2013|      1|     0|            0|             0|     1|        0|      0|       1|\n",
      "|1228863|  2013-01-01|terça-feira|01:30:00| SC|101|  128|     ITAJAI|Não guardar distâ...|    Colisão traseira|           Sem Vítimas|Plena noite|Decrescente|               Nublado|     Dupla|       Reta|   Rural|2013|      3|     0|            0|             0|     3|        0|      0|       3|\n",
      "|1228868|  2013-01-01|terça-feira|00:15:00| PB|101| 92.9|JOAO PESSOA|              Outras|Colisão com objet...|           Sem Vítimas|Plena noite|Decrescente|                 Chuva|     Dupla|      Curva|  Urbano|2013|      1|     0|            0|             0|     1|        0|      0|       1|\n",
      "|1228869|  2013-01-01|terça-feira|01:15:00| SC|101|158.1| PORTO BELO|Não guardar distâ...|    Colisão traseira|   Com Vítimas Feridas|Plena noite|Decrescente|             Ceu Claro|     Dupla|      Curva|  Urbano|2013|      6|     0|            0|             3|     1|        2|      3|       5|\n",
      "+-------+------------+-----------+--------+---+---+-----+-----------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----+-------+------+-------------+--------------+------+---------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Dados de pessoas carregados:\n",
      "+-------+-------+------------+----------+--------+---+---+---+---------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----------+------------+--------------------+----------------------+--------------+-------------+-----+---------+-------------+--------------------+\n",
      "|     id|  pesid|data_inversa|dia_semana| horario| uf| br| km|municipio|      causa_acidente|       tipo_acidente|classificacao_acidente|   fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo|id_veiculo|tipo_veiculo|               marca|ano_fabricacao_veiculo|tipo_envolvido|estado_fisico|idade|     sexo|nacionalidade|        naturalidade|\n",
      "+-------+-------+------------+----------+--------+---+---+---+---------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----------+------------+--------------------+----------------------+--------------+-------------+-----+---------+-------------+--------------------+\n",
      "|1228858|3831812|  01/01/2013|   Terça  |01:00:00| PA|163|992| SANTAREM|Defeito mecânico ...|Queda de motocicl...|   Com Vítimas Feridas|Plena noite|Crescente  |               Nublado|  Simples | Curva     |  Rural |   3091102|Motocicletas|              (null)|                  2008|      Condutor| Ferido Leve |   21|Masculino|       Brasil|       Não Informado|\n",
      "|1228858|3831816|  01/01/2013|   Terça  |01:00:00| PA|163|992| SANTAREM|Defeito mecânico ...|Queda de motocicl...|   Com Vítimas Feridas|Plena noite|Crescente  |               Nublado|  Simples | Curva     |  Rural |   3091102|Motocicletas|              (null)|                  2008|    Passageiro| Ferido Leve |   -1|Masculino|       Brasil|       Não Informado|\n",
      "|1228860|3825791|  01/01/2013|   Terça  |01:00:00| PR|476|114|  COLOMBO|Velocidade incomp...|Colisão com objet...|   Sem Vítimas        |Plena noite|Decrescente|               Nublado|  Simples | Curva     |  Urbano|   3091036|   Automóvel|FIAT/UNO ELECTRON...|                  1993|      Condutor| Ileso       |   49|Masculino|       Brasil|       Não Informado|\n",
      "|1228863|3827778|  01/01/2013|   Terça  |01:30:00| SC|101|128|   ITAJAI|Não guardar distâ...|    Colisão traseira|   Sem Vítimas        |Plena noite|Decrescente|               Nublado|  Dupla   | Reta      |  Rural |   3091037|   Automóvel|I/FORD FOCUS 2.0L HA|                  2009|      Condutor| Ileso       |   -1|Masculino|       Brasil|Sao Francisco Do Sul|\n",
      "|1228863|3827789|  01/01/2013|   Terça  |01:30:00| SC|101|128|   ITAJAI|Não guardar distâ...|    Colisão traseira|   Sem Vítimas        |Plena noite|Decrescente|               Nublado|  Dupla   | Reta      |  Rural |   3091038|   Automóvel|RENAULT/SANDERO E...|                  2012|      Condutor| Ileso       |   20|Masculino|       Brasil|              Itajai|\n",
      "+-------+-------+------------+----------+--------+---+---+---+---------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----------+------------+--------------------+----------------------+--------------+-------------+-----+---------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Dados após o join:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+--------+---+---+----+----------+--------------------+----------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----+-------+------+-------------+--------------+------+---------+-------+--------+-------+------------+----------+--------+---+---+---+----------+--------------------+----------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----------+------------+--------------------+----------------------+--------------+-------------+-----+---------+-------------+-------------+\n",
      "|     id|data_inversa|  dia_semana| horario| uf| br|  km| municipio|      causa_acidente|   tipo_acidente|classificacao_acidente|   fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo| ano|pessoas|mortos|feridos_leves|feridos_graves|ilesos|ignorados|feridos|veiculos|  pesid|data_inversa|dia_semana| horario| uf| br| km| municipio|      causa_acidente|   tipo_acidente|classificacao_acidente|   fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo|id_veiculo|tipo_veiculo|               marca|ano_fabricacao_veiculo|tipo_envolvido|estado_fisico|idade|     sexo|nacionalidade| naturalidade|\n",
      "+-------+------------+------------+--------+---+---+----+----------+--------------------+----------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----+-------+------+-------------+--------------+------+---------+-------+--------+-------+------------+----------+--------+---+---+---+----------+--------------------+----------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----------+------------+--------------------+----------------------+--------------+-------------+-----+---------+-------------+-------------+\n",
      "|1000015|  2011-10-13|quinta-feira|06:55:00| DF| 20|14.0|  BRASILIA|Não guardar distâ...|Colisão traseira|           Sem Vítimas|  Pleno dia|Decrescente|             Ceu Claro|     Dupla|       Reta|   Rural|2011|      3|     0|            0|             0|     3|        0|      0|       3|3063953|  13/10/2011|   Quinta |06:55:00| DF| 20| 14|  BRASILIA|Não guardar distâ...|Colisão traseira|   Sem Vítimas        |  Pleno dia|Decrescente|             Ceu Claro|  Dupla   | Reta      |  Rural |   2689161|   Automóvel|          VW/GOL 1.0|                  2010|      Condutor| Ileso       |   31|Masculino|       Brasil|Não Informado|\n",
      "|1000015|  2011-10-13|quinta-feira|06:55:00| DF| 20|14.0|  BRASILIA|Não guardar distâ...|Colisão traseira|           Sem Vítimas|  Pleno dia|Decrescente|             Ceu Claro|     Dupla|       Reta|   Rural|2011|      3|     0|            0|             0|     3|        0|      0|       3|3063960|  13/10/2011|   Quinta |06:55:00| DF| 20| 14|  BRASILIA|Não guardar distâ...|Colisão traseira|   Sem Vítimas        |  Pleno dia|Decrescente|             Ceu Claro|  Dupla   | Reta      |  Rural |   2689166|   Automóvel|          VW/FOX 1.0|                  2006|      Condutor| Ileso       |   26|Masculino|       Brasil|Não Informado|\n",
      "|1000015|  2011-10-13|quinta-feira|06:55:00| DF| 20|14.0|  BRASILIA|Não guardar distâ...|Colisão traseira|           Sem Vítimas|  Pleno dia|Decrescente|             Ceu Claro|     Dupla|       Reta|   Rural|2011|      3|     0|            0|             0|     3|        0|      0|       3|3067199|  13/10/2011|   Quinta |06:55:00| DF| 20| 14|  BRASILIA|Não guardar distâ...|Colisão traseira|   Sem Vítimas        |  Pleno dia|Decrescente|             Ceu Claro|  Dupla   | Reta      |  Rural |   2689158|   Automóvel|              (null)|                  2003|      Condutor| Ileso       |   -1|Masculino|Não Informado|Não Informado|\n",
      "|1000067|  2011-10-28| sexta-feira|19:20:00| PA|316| 6.0|ANANINDEUA|              Outras|Colisão traseira|           Sem Vítimas|Plena noite|Decrescente|               Nublado|     Dupla|       Reta|  Urbano|2011|      2|     0|            0|             0|     2|        0|      0|       2|3064253|  28/10/2011|   Sexta  |19:20:00| PA|316|  6|ANANINDEUA|              Outras|Colisão traseira|   Sem Vítimas        |Plena noite|Decrescente|               Nublado|  Dupla   | Reta      |  Urbano|   2689331| Caminhonete|FIAT/STRADA ADVEN...|                  2009|      Condutor| Ileso       |   41|Masculino|       Brasil|      Penalva|\n",
      "|1000067|  2011-10-28| sexta-feira|19:20:00| PA|316| 6.0|ANANINDEUA|              Outras|Colisão traseira|           Sem Vítimas|Plena noite|Decrescente|               Nublado|     Dupla|       Reta|  Urbano|2011|      2|     0|            0|             0|     2|        0|      0|       2|3064258|  28/10/2011|   Sexta  |19:20:00| PA|316|  6|ANANINDEUA|              Outras|Colisão traseira|   Sem Vítimas        |Plena noite|Decrescente|               Nublado|  Dupla   | Reta      |  Urbano|   2689333|    Caminhão|VW/7.90 S        ...|                  1987|      Condutor| Ileso       |   41|Masculino|       Brasil|        Belem|\n",
      "+-------+------------+------------+--------+---+---+----+----------+--------------------+----------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----+-------+------+-------------+--------------+------+---------+-------+--------+-------+------------+----------+--------+---+---+---+----------+--------------------+----------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+----------+------------+--------------------+----------------------+--------------+-------------+-----+---------+-------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# CORRIGIDO: O caminho base do seu projeto dentro do contêiner jupyterlab é /app\n",
    "sys.path.append(\"/app\")\n",
    "\n",
    "from utils.dataloader import DataLoader\n",
    "from utils.workload import Workload\n",
    "\n",
    "# --- As definições de função permanecem as mesmas ---\n",
    "def executar_load(spark, base_dir):\n",
    "    loader = DataLoader(spark=spark, base_dir=base_dir)\n",
    "    df = loader.load()\n",
    "    return df\n",
    "\n",
    "def executar_join(df_pair):\n",
    "    df1, df2 = df_pair\n",
    "    df_joined =  .join(df1, df2, on=[\"id\"], how=\"inner\")\n",
    "    return df_joined\n",
    "\n",
    "# --- As definições de workload permanecem as mesmas ---\n",
    "workload_load_ocorrencias = Workload(\n",
    "    name=\"Carga Ocorrências\",\n",
    "    description=\"[WORKLOAD-1] Carregamento dos dados de ocorrências\",\n",
    "    execute_fn=executar_load\n",
    ")\n",
    "workload_load_pessoas = Workload(\n",
    "    name=\"Carga Pessoas\",\n",
    "    description=\"[WORKLOAD-2] Carregamento dos dados de pessoas\",\n",
    "    execute_fn=executar_load\n",
    ")\n",
    "workload_join = Workload(\n",
    "    name=\"Join Ocorrências + Pessoas\",\n",
    "    description=\"[WORKLOAD-3] Junção entre ocorrências e pessoas por ID\",\n",
    "    execute_fn=executar_join\n",
    ")\n",
    "\n",
    "\n",
    "# --- EXECUÇÃO DAS WORKLOADS DE CARREGAMENTO E JUNÇÃO ---\n",
    "\n",
    "# CORRIGIDO: Use os caminhos que existem dentro do contêiner jupyterlab\n",
    "base_ocorrencias = \"/app/full_data/ocorrencias\"\n",
    "base_pessoas     = \"/app/full_data/pessoas\"\n",
    "\n",
    "# O resto do código executa normalmente\n",
    "df_ocorrencias = workload_load_ocorrencias.run(spark, base_ocorrencias)\n",
    "df_pessoas     = workload_load_pessoas.run(spark, base_pessoas)\n",
    "df_joined      = workload_join.run((df_ocorrencias, df_pessoas))\n",
    "\n",
    "# Para verificar se funcionou, você pode mostrar algumas linhas\n",
    "print(\"Dados de ocorrências carregados:\")\n",
    "df_ocorrencias.show(5)\n",
    "\n",
    "print(\"\\nDados de pessoas carregados:\")\n",
    "df_pessoas.show(5)\n",
    "\n",
    "print(\"\\nDados após o join:\")\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0874a-bc78-477c-946a-fd8a75a3ebce",
   "metadata": {},
   "source": [
    "### Distribuição Etária dos Envolvidos em Acidentes de Trânsito\n",
    "\n",
    "O código tem por objetivo realizar uma análise quantitativa da distribuição de indivíduos envolvidos em acidentes de trânsito, segmentando-os por faixas etárias. Inicialmente, é definida uma função que recebe um `DataFrame` do PySpark e cria uma nova coluna denominada `faixa_etaria`, a partir da classificação da variável `idade` em intervalos predefinidos: menores de 18 anos (`<18`), de 18 a 29 anos (`18–29`), de 30 a 44 anos (`30–44`), de 45 a 59 anos (`45–59`) e 60 anos ou mais (`60+`).\n",
    "\n",
    "A seguir, o conjunto de dados é agregado com base nessas faixas etárias, contabilizando o número de registros em cada grupo. Os resultados são convertidos para um `DataFrame` do pandas com o intuito de gerar uma visualização gráfica por meio de um gráfico de barras, que apresenta de forma clara a distribuição dos envolvidos por faixa etária. A função é encapsulada em uma `Workload`, permitindo sua integração no fluxo analítico do projeto, favorecendo a modularização, reutilização e padronização das análises executadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87f891f-832c-480e-ba19-47aeb4d2153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WORKLOAD-4] Analisa o número de envolvidos em acidentes por faixa etária (Distribuição por Faixa Etária)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 05:31:38 WARN TaskSetManager: Lost task 1.0 in stage 12.0 (TID 59) (172.18.0.3 executor 0): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 10 in cell [4]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "25/07/09 05:31:38 ERROR TaskSetManager: Task 1 in stage 12.0 failed 4 times; aborting job\n",
      "25/07/09 05:31:38 WARN TaskSetManager: Lost task 0.1 in stage 12.0 (TID 64) (172.18.0.3 executor 0): TaskKilled (Stage cancelled: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 10 in cell [4]\n",
      ")\n",
      "25/07/09 05:31:39 WARN TaskSetManager: Lost task 2.3 in stage 12.0 (TID 67) (172.18.0.3 executor 0): TaskKilled (Stage cancelled: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 10 in cell [4]\n",
      ")\n",
      "{\"ts\": \"2025-07-09 05:31:39.051\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'NA' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 10 in cell [4]\", \"line\": \"\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o188.collectToPython.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'NA' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\nline 10 in cell [4]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.11/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "[CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 10 in cell [4]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNumberFormatException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     45\u001b[39m workload_faixa_etaria = Workload(\n\u001b[32m     46\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mDistribuição por Faixa Etária\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33m[WORKLOAD-4] Analisa o número de envolvidos em acidentes por faixa etária\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m     execute_fn=analise_faixa_etaria\n\u001b[32m     49\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Execução da workload\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mworkload_faixa_etaria\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pessoas\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/src/utils/workload.py:25\u001b[39m, in \u001b[36mWorkload.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[32m     23\u001b[39m start = perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m elapsed_time = perf_counter() - start\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[METRIC] Tempo de execução (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m segundos\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36manalise_faixa_etaria\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     14\u001b[39m df_faixa_etaria = df_limpo.withColumn(\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfaixa_etaria\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     when(col(\u001b[33m\"\u001b[39m\u001b[33midade_numerica\u001b[39m\u001b[33m\"\u001b[39m) < \u001b[32m18\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m<18\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     .otherwise(\u001b[33m\"\u001b[39m\u001b[33mDesconhecida\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Agrupa por faixa etária\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m resultado = \u001b[43mdf_faixa_etaria\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfaixa_etaria\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfaixa_etaria\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Verificação\u001b[39;00m\n\u001b[32m     29\u001b[39m total_grafico = resultado[\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m].sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:1792\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mPandasDataFrameLike\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPandasConversionMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:197\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n\u001b[32m    199\u001b[39m     pdf = pd.DataFrame.from_records(\n\u001b[32m    200\u001b[39m         rows, index=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns=\u001b[38;5;28mself\u001b[39m.columns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNumberFormatException\u001b[39m: [CAST_INVALID_INPUT] The value 'NA' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 10 in cell [4]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import IntegerType # Importe o tipo Integer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analise_faixa_etaria(df):\n",
    "    \n",
    "    # --- ETAPA DE LIMPEZA ---\n",
    "    # 1. Tenta converter a coluna 'idade' para um número inteiro.\n",
    "    #    Valores como 'NA' se tornarão NULL.\n",
    "    df_limpo = df.withColumn(\"idade_numerica\", col(\"idade\").cast(IntegerType()))\n",
    "\n",
    "    # --- ETAPA DE ANÁLISE (usando a coluna limpa) ---\n",
    "    # 2. Cria a coluna de faixa etária usando a nova coluna numérica.\n",
    "    df_faixa_etaria = df_limpo.withColumn(\n",
    "        \"faixa_etaria\",\n",
    "        when(col(\"idade_numerica\") < 18, \"<18\")\n",
    "        .when((col(\"idade_numerica\") >= 18) & (col(\"idade_numerica\") < 30), \"18-29\")\n",
    "        .when((col(\"idade_numerica\") >= 30) & (col(\"idade_numerica\") < 45), \"30-44\")\n",
    "        .when((col(\"idade_numerica\") >= 45) & (col(\"idade_numerica\") < 60), \"45-59\")\n",
    "        .when(col(\"idade_numerica\") >= 60, \"60+\")\n",
    "        # O otherwise agora pega todos os casos de idade_numerica = NULL (os 'NA' originais)\n",
    "        .otherwise(\"Desconhecida\")\n",
    "    )\n",
    "\n",
    "    # Agrupa por faixa etária\n",
    "    resultado = df_faixa_etaria.groupBy(\"faixa_etaria\").count().orderBy(\"faixa_etaria\").toPandas()\n",
    "    \n",
    "    # Verificação\n",
    "    total_grafico = resultado[\"count\"].sum()\n",
    "    total_pessoas = df.select(\"pesid\").count() # Usa o df original para contagem total\n",
    "    print(f\"[VERIFICAÇÃO] Total no gráfico: {total_grafico}\")\n",
    "    print(f\"[VERIFICAÇÃO] Total de pessoas: {total_pessoas}\")\n",
    "     \n",
    "    # Plot (sem alterações)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(resultado[\"faixa_etaria\"], resultado[\"count\"], color='gray')\n",
    "    plt.xlabel(\"Faixa Etária\")\n",
    "    plt.ylabel(\"Número de Envolvidos\")\n",
    "    plt.title(\"Distribuição de Envolvidos em Acidentes por Faixa Etária\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Criação da workload (sem alterações)\n",
    "workload_faixa_etaria = Workload(\n",
    "    name=\"Distribuição por Faixa Etária\",\n",
    "    description=\"[WORKLOAD-4] Analisa o número de envolvidos em acidentes por faixa etária\",\n",
    "    execute_fn=analise_faixa_etaria\n",
    ")\n",
    "\n",
    "# Execução da workload\n",
    "workload_faixa_etaria.run(df_pessoas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f665c-091a-441b-bbbf-e358a22c8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.workload import Workload\n",
    "from pyspark.sql.functions import when, col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analise_distribuição_genero(df):\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"sexo\",\n",
    "        when(col(\"sexo\").isin(\"Masculino\", \"M\"), \"Masculino\")\n",
    "        .when(col(\"sexo\").isin(\"Feminino\", \"F\"), \"Feminino\")\n",
    "        .when(col(\"sexo\").isin(\"0\", \"NA\", \"Não Informado\", \"Inválido\", \"\", None, \"-1\"), \"Não Informado\")\n",
    "        .otherwise(\"Desconhecido\")\n",
    "    )\n",
    "\n",
    "    # Agrupa por sexo\n",
    "    resultado = df.groupBy(\"sexo\").count().orderBy(\"sexo\").toPandas()\n",
    "\n",
    "    # Verificação\n",
    "    total_grafico = resultado[\"count\"].sum()\n",
    "    total_pessoas = df.select(\"pesid\").count()\n",
    "    print(f\"[VERIFICAÇÃO] Total no gráfico: {total_grafico}\")\n",
    "    print(f\"[VERIFICAÇÃO] Total de pessoas: {total_pessoas}\")\n",
    "\n",
    "    cores_personalizadas = {\n",
    "        \"Masculino\": \"#0467ae\",       # Azul\n",
    "        \"Feminino\": \"#af1c1c\",        # Vermelho\n",
    "        \"Não Informado\": \"#c0c0c0\",   # Cinza claro\n",
    "        \"Desconhecido\": \"#696969\"     # Cinza escuro\n",
    "    }\n",
    "    \n",
    "    # Garante que a ordem dos rótulos siga a ordem correta das cores\n",
    "    labels_ordenadas = resultado[\"sexo\"].tolist()\n",
    "    cores_ordenadas = [cores_personalizadas.get(label, \"#999999\") for label in labels_ordenadas]\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(\n",
    "        resultado[\"count\"],\n",
    "        labels=labels_ordenadas,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        colors=cores_ordenadas\n",
    "    )\n",
    "    plt.title(\"Distribuição de Envolvidos em Acidentes por Gênero\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Criação da workload\n",
    "workload_genero = Workload(\n",
    "    name=\"Distribuição de Envolvidos em Acidentes por Gênero\",\n",
    "    description=\"[WORKLOAD-5] Analisa o número de envolvidos em acidentes por gênero\",\n",
    "    execute_fn=analise_distribuição_genero\n",
    ")\n",
    "\n",
    "# Execução da workload\n",
    "workload_genero.run(df_pessoas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419bb22a-f926-4777-91fa-c64a61aee6a8",
   "metadata": {},
   "source": [
    "### Análise Temporal de Acidentes: Distribuição por Hora do Dia\n",
    "\n",
    "A análise de distribuição de acidentes por hora do dia tem como objetivo identificar a frequência com que os acidentes de trânsito ocorrem ao longo das 24 horas do dia. Esta análise é realizada com base no campo `horario` presente na base de dados consolidada de acidentes, que inclui registros de diversos anos.\n",
    "\n",
    "Para isso, foi definida uma carga de trabalho (`Workload`) que utiliza PySpark para realizar a transformação dos dados, extraindo a hora de cada ocorrência e agrupando os registros por esse valor. O número de acidentes por hora é então contado e ordenado para facilitar a interpretação.\n",
    "\n",
    "Após a etapa de processamento, os resultados são convertidos para um DataFrame pandas, permitindo a geração de um gráfico de barras com a biblioteca Matplotlib. Esse gráfico ilustra visualmente como os acidentes se distribuem ao longo do dia, possibilitando a identificação de faixas horárias com maior ou menor incidência de ocorrências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd5c51-5626-4901-ab16-592fd1f0d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, col, count, year\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def analise_distribuicao_temporal(df, tipo=\"hora\"):\n",
    "    \"\"\"\n",
    "    Realiza a análise da distribuição temporal de acidentes de trânsito,\n",
    "    permitindo agrupar os dados por hora do dia, dia da semana ou ano.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df (DataFrame): DataFrame contendo os dados de ocorrências.\n",
    "    - tipo (str): Tipo de agrupamento temporal. Pode ser 'hora', 'semana' ou 'ano'.\n",
    "    \"\"\"\n",
    "\n",
    "    if tipo == \"hora\":\n",
    "        df_temp = df.withColumn(\"tempo\", hour(\"horario\"))\n",
    "        titulo = \"Distribuição Temporal de Acidentes por Hora do Dia\"\n",
    "        eixo_x = \"Hora\"\n",
    "    elif tipo == \"semana\":\n",
    "        df_temp = df.withColumn(\"tempo\", col(\"dia_semana\"))\n",
    "        titulo = \"Distribuição Temporal de Acidentes por Dia da Semana\"\n",
    "        eixo_x = \"Dia da Semana\"\n",
    "    elif tipo == \"ano\":\n",
    "        df_temp = df.withColumn(\"tempo\", year(col(\"data_inversa\")))\n",
    "        titulo = \"Distribuição Temporal de Acidentes por Ano\"\n",
    "        eixo_x = \"Ano\"\n",
    "    else:\n",
    "        raise ValueError(\"O parâmetro 'tipo' deve ser 'hora', 'semana' ou 'ano'.\")\n",
    "\n",
    "    # Agrupa e converte para pandas\n",
    "    resultado = df_temp.groupBy(\"tempo\").agg(count(\"*\").alias(\"total\")).orderBy(\"tempo\").toPandas()\n",
    "\n",
    "    # Ordenação dos dias da semana, se necessário\n",
    "    if tipo == \"semana\":\n",
    "        ordem_dias = [\n",
    "            \"segunda-feira\", \"terça-feira\", \"quarta-feira\",\n",
    "            \"quinta-feira\", \"sexta-feira\", \"sábado\", \"domingo\"\n",
    "        ]\n",
    "        resultado[\"tempo\"] = pd.Categorical(resultado[\"tempo\"], categories=ordem_dias, ordered=True)\n",
    "        resultado = resultado.sort_values(\"tempo\")\n",
    "\n",
    "    # Geração do gráfico\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.bar(resultado[\"tempo\"], resultado[\"total\"], color=\"gray\")\n",
    "    plt.xlabel(eixo_x)\n",
    "    plt.ylabel(\"Número de Acidentes\")\n",
    "    plt.title(titulo)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    if tipo == \"ano\":\n",
    "        plt.xticks(resultado[\"tempo\"][::1], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f82c37-1e1c-457e-b364-5129314c0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.workload import Workload\n",
    "\n",
    "# Cria uma workload para distribuição por hora\n",
    "workload_temporal_hora = Workload(\n",
    "    name=\"Distribuição Temporal de Acidentes por Hora do Dia\",\n",
    "    description=\"[WORKLOAD-6] Análise Temporal de Acidentes: Distribuição por Hora do Dia\",\n",
    "    execute_fn=lambda df: analise_distribuicao_temporal(df, tipo=\"hora\")\n",
    ")\n",
    "\n",
    "workload_temporal_hora.run(df_ocorrencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3366ef-902f-41a1-afa9-c1e662966779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.workload import Workload\n",
    "\n",
    "# Cria uma workload para distribuição por dia da semana\n",
    "workload_temporal_hora = Workload(\n",
    "    name=\"Distribuição Temporal de Acidentes por Dia da Semana\",\n",
    "    description=\"[WORKLOAD-7] Análise Temporal de Acidentes: Distribuição por Dia da Semana\",\n",
    "    execute_fn=lambda df: analise_distribuicao_temporal(df, tipo=\"semana\")\n",
    ")\n",
    "\n",
    "workload_temporal_hora.run(df_ocorrencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb7588-ea70-4ba9-9386-260bc3938a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.workload import Workload\n",
    "\n",
    "# Cria uma workload para distribuição por ano\n",
    "workload_temporal_hora = Workload(\n",
    "    name=\"Distribuição Temporal de Acidentes por Ano\",\n",
    "    description=\"[WORKLOAD-8] Análise Temporal de Acidentes: Distribuição por Ano\",\n",
    "    execute_fn=lambda df: analise_distribuicao_temporal(df, tipo=\"ano\")\n",
    ")\n",
    "\n",
    "workload_temporal_hora.run(df_ocorrencias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
