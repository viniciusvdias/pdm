# Relat√≥rio Final do Projeto: An√°lise de Constrained-Off E√≥lico

## 1. Contexto e motiva√ß√£o

### üìã Descri√ß√£o

Este projeto implementa uma solu√ß√£o completa de big-data para an√°lise de constrained-off (restri√ß√µes de gera√ß√£o) em usinas e√≥licas brasileiras, utilizando dados consolidados do ONS (Operador Nacional do Sistema El√©trico).

### üéØ Objetivos

1. **Processamento** dos dados consolidados via lotes.
2. **Agrega√ß√µes temporais e espaciais** com convers√£o para formato Parquet.
3. **Detec√ß√£o de padr√µes e anomalias** nos eventos de constrained-off.
4. **Visualiza√ß√£o interativa** das an√°lises.
5. **Avalia√ß√£o da arquitetura** proposta.

## 2. Data

### 2.1 Descri√ß√£o detalhada

O dataset utilizado consiste em dois tipos complementares: **principal** e **detalhamento**.

<ins>Observa√ß√£o</ins>: As informa√ß√µes s√£o associadas √† apura√ß√£o das restri√ß√µes de opera√ß√£o por constrained-off nas usinas e√≥licas classificadas nas modalidades Tipo I, Tipo II-B e Tipo II-C.

Os dados s√£o abertos e disponibilizados pela ONS, o qual pode ser acess√≠vel pelo link: https://dados.ons.org.br/.

Abaixo est√° representado uma descri√ß√£o breve, e ap√≥s, o cabe√ßalho de cada estrutura.

### üìä Dados de Entrada (Consolidados)
- **Arquivo Principal**: `restricoes_coff_eolicas_consolidado.csv`
  - <ins>Defini√ß√£o</ins>: representa a gera√ß√£o, disponibilidade e restri√ß√µes por usina.
  - Apresenta os dados consolidados de todos os meses com uma granularidade de 30 minutos.

- **Arquivo de Detalhamento**: `restricoes_coff_eolicas_detalhamento_consolidado.csv`
  - <ins>Defini√ß√£o</ins>: representa os dados de vento, gera√ß√£o estimada vs verificada.
  - As informa√ß√µes s√£o por unidade geradora.

Os arquivos originais s√£o hist√≥ricos mensais, tal que h√° ~1.000.000 registros por arquivo, resultando em ~8GB de dados (considerando 42 meses de dados).

### üß© Cabe√ßalho

- Presente em ambas estruturas:
  - **id_subsistema**: Identificador do Subsistema.
  - **id_estado**: Sigla do Estado.
  - **nom_usina**: Nome da Usina ou Conjunto de Usinas.
  - **id_ons**: Identificador da Usina ou do Conjunto de Usina no ONS.
  - **ceg**: C√≥digo √önico do Empreendimento de Gera√ß√£o (CEG), estabelecido pela ANEEL.
  - **din_instante**: Data/Hora.

- Presente somente na estrutura **principal**:
  - **nom_subsistema**: Nome do Subsistema.
  - **nom_estado**: Nome do Estado.
  - **val_geracao**: Valor da Gera√ß√£o, em MWmed.
  - **val_geracaolimitada**: Valor da Gera√ß√£o Limitada por alguma Restri√ß√£o, em MWmed.
  - **val_disponibilidade**: Valor da Disponibilidade Verificada no Tempo Real, em MWmed.
  - **val_geracaoreferencia**: Valor da Gera√ß√£o de refer√™ncia (ou estimada), em MWmed.
  - **val_geracaoreferenciafinal**: Valor da Gera√ß√£o de Refer√™ncia Final, em MWmed.
  - **cod_razaorestricao**: C√≥digo da Raz√£o da Restri√ß√£o, podendo ser:
    - **REL** ‚Äì Raz√£o de indisponibilidade externa (el√©trica).
    - **CNF** ‚Äì Raz√£o de atendimento a requisitos de confiabilidade.
    - **ENE** ‚Äì Raz√£o energ√©tica.
    - **PAR** - Restri√ß√£o indicada no parecer de acesso.
  - **cod_origemrestricao**: C√≥digo da Origem da Restri√ß√£o, podendo ser:
    - **LOC** ‚Äì Local.
    - **SIS** ‚Äì Sist√™mica.

- Presente somente na estrutura **detalhamento**:
  - **nom_modalidadeoperacao**: Modalidade de Opera√ß√£o da Usina.
  - **nom_conjuntousina**: Nome do Conjunto de Usina (apenas para usinas com modalidade de opera√ß√£o Tipo II-C)
  - **val_ventoverificado**: Vento verificado, em m<sup>3</sup>/s.
  - **flg_dadoventoinvalido**: Indicativo do dado de vento inv√°lido (quando flag = 1).
  - **val_geracaoestimada**: Valor da gera√ß√£o estimada da usina, em MWmed.
  - **val_geracaoverificada**: Gera√ß√£o verificada da usina, em MWmed.

### üóìÔ∏è Per√≠odo de An√°lise
  - **In√≠cio**: Outubro/2021
  - **Fim**: Abril/2025
  - **Total**: 42 meses de dados.
  - **Volume**: Dados consolidados de constrained-off e√≥lico.

### 2.3 Como obter os dados

Uma amostra (dois arquivos complementares, totalizando um pouco mais de 512KB) est√° inclu√≠da no diret√≥rio `datasample/` para a execu√ß√£o de testes r√°pidos e demonstra√ß√£o do projeto funcionando.

Para o dataset completo, mais informa√ß√µes ser√£o apresentadas posteriormente, mas basicamente pode-se obt√™-lo via execu√ß√£o de c√©lulas dos notebooks Jupyter do projeto, previamente j√° configuradas com o link p√∫blico via Google Drive.

## 3. Como instalar e executar

> Observa√ß√£o: O projeto √© compat√≠vel com Docker e n√£o requer ferramentas ou instala√ß√µes adicionais al√©m do necess√°rio.

Para uso da amostra ou do dataset completo, a inicializa√ß√£o √© a mesma.

Para executar o projeto, clone este reposit√≥rio e acesse a pasta `/g7` via terminal a fim de utilizar:

```bash
./bin/run.sh
```

Este comando ir√° automaticamente criar a imagem do container `jupytercli-g7`, al√©m de realizar o deploy, via swarm, do `spark-master-g7` e do(s) `spark-worker-g7`.

Ao final da inicializa√ß√£o, √© disponibilizado tr√™s links:
- JupyterLab:    http://localhost:8888 + (token)
- Spark Master:  http://localhost:8080
- Spark UI:      http://localhost:4040

Fica a crit√©rio do usu√°rio acessar as refer√™ncias do Spark Master e UI, por√©m √© essencial acessar a refer√™ncia do JupyterLab para a execu√ß√£o do projeto.

√â de extrema import√¢ncia que, ap√≥s o usu√°rio executar o projeto e realizar suas an√°lises, seja efetuado o seguinte comando:

```bash
./bin/run.sh clean
```

Isso remove todos os containers, volumes, redes e o Jupyter (imagem √© mantida) do projeto.

Caso seja de curiosidade do usu√°rio, recomenda-se observar os outros comandos dispon√≠veis pelo c√≥digo:

```bash
./bin/run.sh help
```

Para o restante da execu√ß√£o, o usu√°rio **DEVE** clicar na refer√™ncia do JupyterLab dispon√≠vel via terminal, depois acessar o notebok `src/Pre-Process.ipynb`.

### 3.1 In√≠cio r√°pido (usando dados de amostra)

No notebook `src/Pre-Process.ipynb`, basta seguir as instru√ß√µes e executar as c√©lulas `Passo 2 - Dados amostrais`.

![c√©lula dados amostrais](images/cel_amostra.png)

### 3.2 Como executar com o dataset inteiro

No notebook `src/Pre-Process.ipynb`, basta seguir as instru√ß√µes e executar as c√©lulas `Passo 2 - Dataset Completo`.

![c√©lulas dataset completo](images/cel_dataset.png)

### 3.3 (Opcional) Execu√ß√£o na VM

Durante a disciplina foi disponibilizado m√°quina virtuais para os grupos trabalharem seus projetos dentro delas. Portanto, para acessar a VM destinada a este grupo, pode-se executar o comando:

```bash
./bin/deploy-to-vm.sh
```

As credenciais ser√£o solicitadas, e automaticamente o usu√°rio ser√° direcionado ao terminal da VM, onde j√° ter√° dispon√≠vel os arquivos necess√°rios para a execu√ß√£o do projeto conforme os passos deste t√≥pico.

## 4. Arquitetura do projeto

O projeto utiliza uma arquitetura baseada no pr√©-processamento utilizando Apache Spark, convertendo os resultados para a extens√£o Parquet, a fim de se realizar as an√°lises de constrained-off via SparkSQL.

![diagrama arquitetura do projeto](images/diagrama.jpg)

```
Consolidated CSV Files ‚Üí Parquet Processing ‚Üí Temporal/Spatial Aggregations ‚Üí Anomaly Detection ‚Üí Visualization
```

### Componentes Principais

A. **Pr√©-Processamento** (`Pre-Process.ipynb`)
   - Carregamento de dados consolidados.
   - Limpeza e normaliza√ß√£o.
   - Convers√£o para Parquet.

B. **Compara√ß√£o de Desempenho** (`Performance-Benchmark.ipynb`)
  - Compara o tempo de execu√ß√£o entre CSV e Parquet.
  - Analisa o uso de mem√≥ria e throughput.
  - Executa as queries de detec√ß√£o de anomalias em ambos os formatos.
  - Gera os relat√≥rios de performance detalhados com recomenda√ß√µes.

### Fluxo de dados

1. **Ingest√£o**: Carregamento dos dados das usinas (A).
2. **Pr√©-processamento**: Limpeza e normaliza√ß√£o (A).
3. **An√°lise**: Detec√ß√£o de padr√µes constrained-off (B).
4. **Compara√ß√£o**: M√©tricas de performance entre CSV e Parquet e derivados (B).
5. **Sa√≠da**: Relat√≥rios de constrained-off e de performance (B).


## 5. Workloads Avaliados

### [WORKLOAD-1] CARREGAMENTO_DADOS
**Descri√ß√£o**: Carregamento e transforma√ß√µes b√°sicas de dados e√≥licos
- **Opera√ß√µes CSV**: 
  - Leitura de arquivos CSV com delimitador ";"
  - Infer√™ncia de schema
  - Aplica√ß√£o de transforma√ß√µes: c√°lculo de constrained-off, percentual de constrained-off, extra√ß√£o de ano/m√™s/hora
- **Opera√ß√µes Parquet**:
  - Leitura direta de arquivo Parquet
  - Verifica√ß√£o e aplica√ß√£o condicional de transforma√ß√µes
  - Mesmo conjunto de c√°lculos derivados

**M√©trica Principal**: Contagem total de registros processados (9.503.712 registros)

### [WORKLOAD-2] QUERIES_ANOMALIA
**Descri√ß√£o**: Execu√ß√£o de queries complexas para detec√ß√£o de anomalias em dados e√≥licos
- **Query 1 - Constrained-off Extremo**:
  ```sql
  SELECT nom_usina, nom_estado, ano, mes,
         AVG(percentual_constrained) as percentual_medio,
         MAX(percentual_constrained) as percentual_max,
         COUNT(*) as registros
  FROM wind_data
  WHERE percentual_constrained > 50
  GROUP BY nom_usina, nom_estado, ano, mes
  HAVING AVG(percentual_constrained) > 70
  ORDER BY percentual_medio DESC
  ```

- **Query 2 - Padr√µes Temporais**:
  ```sql
  SELECT nom_usina, nom_estado, hora,
         AVG(percentual_constrained) as percentual_medio_hora,
         COUNT(*) as registros_hora
  FROM wind_data
  GROUP BY nom_usina, nom_estado, hora
  HAVING AVG(percentual_constrained) > 30
  ORDER BY percentual_medio_hora DESC
  ```

- **Query 3 - Clusters Espaciais**:
  ```sql
  WITH state_anomalies AS (
      SELECT nom_estado, ano, mes,
             AVG(percentual_constrained) as percentual_estado,
             COUNT(DISTINCT nom_usina) as num_usinas_afetadas
      FROM wind_data
      WHERE percentual_constrained > 30
      GROUP BY nom_estado, ano, mes
  )
  SELECT nom_estado, ano, mes, percentual_estado, num_usinas_afetadas,
         CASE 
             WHEN percentual_estado > 50 AND num_usinas_afetadas > 3 THEN 'CLUSTER_CRITICO'
             WHEN percentual_estado > 30 AND num_usinas_afetadas > 2 THEN 'CLUSTER_MODERADO'
             ELSE 'ISOLADO'
         END as tipo_cluster
  FROM state_anomalies
  WHERE percentual_estado > 30
  ORDER BY percentual_estado DESC, num_usinas_afetadas DESC
  ```

**M√©trica Principal**: Total de registros retornados pelas queries (9.604 registros)

## 6. Experimentos e Resultados

### 6.1 Ambiente Experimental

**Configura√ß√£o do Cluster Spark**:
- **Master Node**: spark-master-g7:7077
- **Configura√ß√µes testadas**:
  - 1 Worker: 2 cores, 2GB RAM, paralelismo 2
  - 2 Workers: 4 cores, 4GB RAM, paralelismo 4  
  - 3 Workers: 6 cores, 6GB RAM, paralelismo 6
- **Configura√ß√µes Spark**:
  - Serializer: KryoSerializer
  - Adaptive Query Execution: habilitado
  - Parquet: compress√£o Snappy, leitor vetorizado
  - CSV: column pruning habilitado

### 6.2 Par√¢metros Testados

**Vari√°veis Independentes**:
- Formato de dados: CSV vs Parquet
- N√∫mero de workers: 1, 2, 3
- Paralelismo: 2, 4, 6 parti√ß√µes
- Tipo de workload: carregamento vs queries complexas

**M√©tricas Coletadas**:
- **Tempo de execu√ß√£o** (segundos)
- **Throughput** (registros/segundo)
- **Uso de mem√≥ria** (GB)
- **Registros processados** (contagem)

**Metodologia**: Cada configura√ß√£o foi executada 2 vezes, calculando-se m√©dia e desvio padr√£o.

### 6.3 Resultados

#### Resumo Geral de Performance

| Workload | Formato | Tempo M√©dio (s) | Throughput (reg/s) | Mem√≥ria (GB) |
|----------|---------|-----------------|-------------------|--------------|
| CARREGAMENTO_DADOS | CSV | 52.32 ¬± 5.95 | 183,639 | 1.13 ¬± 0.58 |
| CARREGAMENTO_DADOS | Parquet | 10.06 ¬± 3.22 | 1,051,694 | 0.68 ¬± 0.30 |
| QUERIES_ANOMALIA | CSV | 112.87 ¬± 16.51 | 86.59 | 1.61 ¬± 0.74 |
| QUERIES_ANOMALIA | Parquet | 25.07 ¬± 5.74 | 402.09 | 1.61 ¬± 0.79 |

#### An√°lise de Speedup

| Workload | Speedup Factor | Melhoria Temporal (%) | Melhoria Throughput (%) |
|----------|----------------|----------------------|-------------------------|
| CARREGAMENTO_DADOS | **5.2x** | 80.8% | 472.7% |
| QUERIES_ANOMALIA | **4.5x** | 77.8% | 364.4% |
| **M√©dia Geral** | **4.8x** | **79.3%** | **418.5%** |

#### Performance por Configura√ß√£o de Cluster

**Carregamento de Dados**:
- 1 Worker: CSV 57.9s ‚Üí Parquet 6.5s (8.9x speedup)
- 2 Workers: CSV 45.9s ‚Üí Parquet 10.3s (4.5x speedup)
- 3 Workers: CSV 53.2s ‚Üí Parquet 13.3s (4.0x speedup)

**Queries de Anomalia**:
- 1 Worker: CSV 131.9s ‚Üí Parquet 18.3s (7.2x speedup)
- 2 Workers: CSV 109.5s ‚Üí Parquet 26.0s (4.2x speedup)
- 3 Workers: CSV 97.2s ‚Üí Parquet 30.9s (3.1x speedup)

#### An√°lise de Escalabilidade

**Efici√™ncia Paralela**:
- **CSV**: Mant√©m 100% de efici√™ncia em configura√ß√µes de 2-4 cores
- **Parquet**: Mant√©m 100% de efici√™ncia em configura√ß√µes de 2-4 cores
- **Observa√ß√£o**: Degrada√ß√£o de performance com 3 workers devido a overhead de coordena√ß√£o

#### Uso de Mem√≥ria

- **Parquet**: 40% menos uso de mem√≥ria no carregamento (0.68GB vs 1.13GB)
- **Queries**: Uso similar de mem√≥ria entre formatos (~1.6GB)
- **Variabilidade**: Parquet apresenta menor variabilidade no uso de mem√≥ria

## 7. Discuss√£o e Conclus√µes

### 7.1 Principais Descobertas

**1. Superioridade Consistente do Parquet**:
- Parquet demonstrou performance consistentemente superior com speedup m√©dio de 4.8x
- Melhoria mais significativa em carregamento de dados (5.2x) vs queries complexas (4.5x)
- Throughput 4-5x maior em todas as configura√ß√µes testadas

**2. Efici√™ncia de Armazenamento e I/O**:
- Parquet reduziu significativamente o tempo de I/O devido √† compress√£o e organiza√ß√£o colunar
- Menor uso de mem√≥ria (40% redu√ß√£o) durante carregamento
- Leitor vetorizado do Parquet otimiza opera√ß√µes em lote

**3. Comportamento de Escalabilidade**:
- Configura√ß√£o √≥tima: 1-2 workers para este dataset
- Overhead de coordena√ß√£o com 3+ workers reduz efici√™ncia
- Parquet mant√©m vantagem em todas as configura√ß√µes de paralelismo

### 7.2 An√°lise dos Workloads

**Carregamento de Dados**:
- Maior benef√≠cio do Parquet (5.2x speedup)
- Schema pr√©-definido elimina overhead de infer√™ncia
- Compress√£o Snappy reduz volume de dados lidos

**Queries de Anomalia**:
- Speedup moderado mas consistente (4.5x)
- Organiza√ß√£o colunar beneficia agrega√ß√µes e filtros
- Pushdown de predicados mais eficiente

### 7.3 Limita√ß√µes e Desafios

**Limita√ß√µes Identificadas**:
1. **Overhead de Coordena√ß√£o**: Performance degrada com mais de 2 workers
2. **Variabilidade**: Maior variabilidade nos tempos do Parquet (desvio padr√£o 3.2s vs 5.9s)
3. **Prepara√ß√£o de Dados**: Transforma√ß√µes condicionais adicionam complexidade

**Desafios T√©cnicos**:
1. **Configura√ß√£o de Cluster**: Balanceamento entre paralelismo e overhead
2. **Gerenciamento de Mem√≥ria**: Necessidade de limpeza entre execu√ß√µes
3. **Consist√™ncia de Resultados**: Variabilidade nos tempos de execu√ß√£o

### 7.4 Recomenda√ß√µes

**Para Workloads Similares**:
1. **Use Parquet** para datasets de produ√ß√£o com +5M registros
2. **Configure 1-2 workers** para datasets de tamanho m√©dio
3. **Implemente cache** para queries repetitivas
4. **Monitore uso de mem√≥ria** em clusters compartilhados

**Para Otimiza√ß√£o Futura**:
1. **Particionamento**: Particionar Parquet por ano/m√™s para queries temporais
2. **Compress√£o**: Testar diferentes algoritmos (LZ4, GZIP)
3. **Bucketing**: Implementar bucketing para joins frequentes
4. **Caching**: Usar cache de dados para queries iterativas

### 7.5 Conclus√£o Final

O benchmark demonstra que **Parquet √© significativamente superior ao CSV** para workloads de big data com Apache Spark, oferecendo:

- **4.8x speedup m√©dio** em performance
- **40% redu√ß√£o** no uso de mem√≥ria
- **Maior throughput** (418% de melhoria)
- **Melhor escalabilidade** em configura√ß√µes distribu√≠das

Para o contexto espec√≠fico de an√°lise de dados e√≥licos, **recomenda-se fortemente a migra√ß√£o para Parquet** como formato de armazenamento padr√£o, especialmente para workloads que envolvem carregamento frequente de dados e queries anal√≠ticas complexas.

A diferen√ßa de performance justifica o investimento em convers√£o de dados e ajustes de pipeline, resultando em economia significativa de recursos computacionais e tempo de processamento.

## 8. Refer√™ncias e recursos externos

- Apache Spark: https://spark.apache.org/
- Documenta√ß√£o PySpark: https://spark.apache.org/docs/latest/api/python/
- Docker: https://www.docker.com/
- Google Drive

- List all external resources, datasets, libraries, and tools you used (with links).
