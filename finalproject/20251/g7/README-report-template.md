# Relat√≥rio Final do Projeto: An√°lise de Constrained-Off E√≥lico

## 1. Contexto e motiva√ß√£o

### üìã Descri√ß√£o

Este projeto implementa uma solu√ß√£o completa de big-data para an√°lise de constrained-off (restri√ß√µes de gera√ß√£o) em usinas e√≥licas brasileiras, utilizando dados consolidados do ONS (Operador Nacional do Sistema El√©trico).

### üéØ Objetivos

1. **Processamento** dos dados consolidados via lotes.
2. **Agrega√ß√µes temporais e espaciais** com convers√£o para formato Parquet.
3. **Detec√ß√£o de padr√µes e anomalias** nos eventos de constrained-off.
4. **Visualiza√ß√£o interativa** das an√°lises.
5. **Avalia√ß√£o da arquitetura** proposta.

## 2. Data

### 2.1 Descri√ß√£o detalhada

O dataset utilizado consiste em dois tipos complementares: **principal** e **detalhamento**.

<ins>Observa√ß√£o</ins>: As informa√ß√µes s√£o associadas √† apura√ß√£o das restri√ß√µes de opera√ß√£o por constrained-off nas usinas e√≥licas classificadas nas modalidades Tipo I, Tipo II-B e Tipo II-C.

Os dados s√£o abertos e disponibilizados pela ONS, o qual pode ser acess√≠vel pelo link: https://dados.ons.org.br/.

Abaixo est√° representado uma descri√ß√£o breve, e ap√≥s, o cabe√ßalho de cada estrutura.

### üìä Dados de Entrada (Consolidados)
- **Arquivo Principal**: `restricoes_coff_eolicas_consolidado.csv`
  - <ins>Defini√ß√£o</ins>: representa a gera√ß√£o, disponibilidade e restri√ß√µes por usina.
  - Apresenta os dados consolidados de todos os meses com uma granularidade de 30 minutos.

- **Arquivo de Detalhamento**: `restricoes_coff_eolicas_detalhamento_consolidado.csv`
  - <ins>Defini√ß√£o</ins>: representa os dados de vento, gera√ß√£o estimada vs verificada.
  - As informa√ß√µes s√£o por unidade geradora.

Os arquivos originais s√£o hist√≥ricos mensais, tal que h√° ~1.000.000 registros por arquivo, resultando em ~8GB de dados (considerando 42 meses de dados).

### üß© Cabe√ßalho

- Presente em ambas estruturas:
  - **id_subsistema**: Identificador do Subsistema.
  - **id_estado**: Sigla do Estado.
  - **nom_usina**: Nome da Usina ou Conjunto de Usinas.
  - **id_ons**: Identificador da Usina ou do Conjunto de Usina no ONS.
  - **ceg**: C√≥digo √önico do Empreendimento de Gera√ß√£o (CEG), estabelecido pela ANEEL.
  - **din_instante**: Data/Hora.

- Presente somente na estrutura **principal**:
  - **nom_subsistema**: Nome do Subsistema.
  - **nom_estado**: Nome do Estado.
  - **val_geracao**: Valor da Gera√ß√£o, em MWmed.
  - **val_geracaolimitada**: Valor da Gera√ß√£o Limitada por alguma Restri√ß√£o, em MWmed.
  - **val_disponibilidade**: Valor da Disponibilidade Verificada no Tempo Real, em MWmed.
  - **val_geracaoreferencia**: Valor da Gera√ß√£o de refer√™ncia (ou estimada), em MWmed.
  - **val_geracaoreferenciafinal**: Valor da Gera√ß√£o de Refer√™ncia Final, em MWmed.
  - **cod_razaorestricao**: C√≥digo da Raz√£o da Restri√ß√£o, podendo ser:
    - **REL** ‚Äì Raz√£o de indisponibilidade externa (el√©trica).
    - **CNF** ‚Äì Raz√£o de atendimento a requisitos de confiabilidade.
    - **ENE** ‚Äì Raz√£o energ√©tica.
    - **PAR** - Restri√ß√£o indicada no parecer de acesso.
  - **cod_origemrestricao**: C√≥digo da Origem da Restri√ß√£o, podendo ser:
    - **LOC** ‚Äì Local.
    - **SIS** ‚Äì Sist√™mica.

- Presente somente na estrutura **detalhamento**:
  - **nom_modalidadeoperacao**: Modalidade de Opera√ß√£o da Usina.
  - **nom_conjuntousina**: Nome do Conjunto de Usina (apenas para usinas com modalidade de opera√ß√£o Tipo II-C)
  - **val_ventoverificado**: Vento verificado, em m<sup>3</sup>/s.
  - **flg_dadoventoinvalido**: Indicativo do dado de vento inv√°lido (quando flag = 1).
  - **val_geracaoestimada**: Valor da gera√ß√£o estimada da usina, em MWmed.
  - **val_geracaoverificada**: Gera√ß√£o verificada da usina, em MWmed.

### üóìÔ∏è Per√≠odo de An√°lise
  - **In√≠cio**: Outubro/2021
  - **Fim**: Abril/2025
  - **Total**: 42 meses de dados.
  - **Volume**: Dados consolidados de constrained-off e√≥lico.

### 2.3 Como obter os dados

Uma amostra (dois arquivos complementares, totalizando um pouco mais de 512KB) est√° inclu√≠da no diret√≥rio `datasample/` para a execu√ß√£o de testes r√°pidos e demonstra√ß√£o do projeto funcionando.

Para o dataset completo, mais informa√ß√µes ser√£o apresentadas posteriormente, mas basicamente pode-se obt√™-lo via execu√ß√£o de c√©lulas dos notebooks Jupyter do projeto, previamente j√° configuradas com o link p√∫blico via Google Drive.

## 3. Como instalar e executar

> Observa√ß√£o: O projeto √© compat√≠vel com Docker e n√£o requer ferramentas ou instala√ß√µes adicionais al√©m do necess√°rio.

Para uso da amostra ou do dataset completo, a inicializa√ß√£o √© a mesma.

Para executar o projeto, clone este reposit√≥rio e acesse a pasta `/g7` via terminal a fim de utilizar:

```bash
./bin/run.sh
```

Este comando ir√° automaticamente criar a imagem do container `jupytercli-g7`, al√©m de realizar o deploy, via swarm, do `spark-master-g7` e do(s) `spark-worker-g7`.

Ao final da inicializa√ß√£o, √© disponibilizado tr√™s links:
- JupyterLab:    http://localhost:8888 + (token)
- Spark Master:  http://localhost:8080
- Spark UI:      http://localhost:4040

Fica a crit√©rio do usu√°rio acessar as refer√™ncias do Spark Master e UI, por√©m √© essencial acessar a refer√™ncia do JupyterLab para a execu√ß√£o do projeto.

√â de extrema import√¢ncia que, ap√≥s o usu√°rio executar o projeto e realizar suas an√°lises, seja efetuado o seguinte comando:

```bash
./bin/run.sh clean
```

Isso remove todos os containers, volumes, redes e o Jupyter (imagem √© mantida) do projeto.

Caso seja de curiosidade do usu√°rio, recomenda-se observar os outros comandos dispon√≠veis pelo c√≥digo:

```bash
./bin/run.sh help
```

Para o restante da execu√ß√£o, o usu√°rio **DEVE** clicar na refer√™ncia do JupyterLab dispon√≠vel via terminal, depois acessar o notebok `src/Pre-Process.ipynb`.

### 3.1 In√≠cio r√°pido (usando dados de amostra)

No notebook `src/Pre-Process.ipynb`, basta seguir as instru√ß√µes e executar as c√©lulas `Passo 2 - Dados amostrais`.

![c√©lula dados amostrais](images/cel_amostra.png)

### 3.2 Como executar com o dataset inteiro

No notebook `src/Pre-Process.ipynb`, basta seguir as instru√ß√µes e executar as c√©lulas `Passo 2 - Dataset Completo`.

![c√©lulas dataset completo](images/cel_dataset.png)

### 3.3 (Opcional) Execu√ß√£o na VM

Durante a disciplina foi disponibilizado m√°quina virtuais para os grupos trabalharem seus projetos dentro delas. Portanto, para acessar a VM destinada a este grupo, pode-se executar o comando:

```bash
./bin/deploy-to-vm.sh
```

As credenciais ser√£o solicitadas, e automaticamente o usu√°rio ser√° direcionado ao terminal da VM, onde j√° ter√° dispon√≠vel os arquivos necess√°rios para a execu√ß√£o do projeto conforme os passos deste t√≥pico.

## 4. Arquitetura do projeto

O projeto utiliza uma arquitetura baseada no pr√©-processamento utilizando Apache Spark, convertendo os resultados para a extens√£o Parquet, a fim de se realizar as an√°lises de constrained-off via SparkSQL.

![diagrama arquitetura do projeto](images/diagrama.jpg)

```
Consolidated CSV Files ‚Üí Parquet Processing ‚Üí Temporal/Spatial Aggregations ‚Üí Anomaly Detection ‚Üí Visualization
```

### Componentes Principais

A. **Pr√©-Processamento** (`Pre-Process.ipynb`)
   - Carregamento de dados consolidados.
   - Limpeza e normaliza√ß√£o.
   - Convers√£o para Parquet.

B. **Compara√ß√£o de Desempenho** (`Performance-Benchmark.ipynb`)
  - Compara o tempo de execu√ß√£o entre CSV e Parquet.
  - Analisa o uso de mem√≥ria e throughput.
  - Executa as queries de detec√ß√£o de anomalias em ambos os formatos.
  - Gera os relat√≥rios de performance detalhados com recomenda√ß√µes.

### Fluxo de dados

1. **Ingest√£o**: Carregamento dos dados das usinas (A).
2. **Pr√©-processamento**: Limpeza e normaliza√ß√£o (A).
3. **An√°lise**: Detec√ß√£o de padr√µes constrained-off (B).
4. **Compara√ß√£o**: M√©tricas de performance entre CSV e Parquet e derivados (B).
5. **Sa√≠da**: Relat√≥rios de constrained-off e de performance (B).

## 5. Workloads evaluated

- Specify each big data task evaluated in your project (queries, data pre-processing, sub-routines, etc.).
- Be specific: describe the details of each workload, and give each a clear name. These named workloads will be referenced and evaluated via performance experiments in the next section.
  - Example: [WORKLOAD-1] Query that computes the average occupation within each
    time window (include query below). [WORKLOAD-2] Preprocessing, including
  removing duplicates, standardization, etc.

## 6. Experiments and results

### 6.1 Experimental environment

- Describe the environment used for experiments (machine/VM specs, OS, Docker version, etc.).
- Example:
  > Experiments were run on a virtual machine with 4 vCPUs, 8GB RAM, Ubuntu 22.04, Docker 24.x.

### 6.2 What did you test?

- What parameters did you try changing? What did you measure (e.g. throughput, latency, wall-clock time, resident memory, disk usage, application level metrics)?
- The ideal is that, for each execution configuration, you repeat the experiments a number of times (replications). With this information, report the average and also the variance/deviation of the results.

### 6.3 Results

- Use tables and plots (insert images).
- Discuss your results: What do they mean? What did you learn about the data or
about the computational cost of processing this data?
- Do not just show numbers or plots ‚Äî always explain what they mean and why they matter.

## 7. Discussion and conclusions

- Summarize what worked and what did not.
- Discuss any challenges or limitations of this work.

## 8. Refer√™ncias e recursos externos

- Apache Spark: https://spark.apache.org/
- Documenta√ß√£o PySpark: https://spark.apache.org/docs/latest/api/python/
- Docker: https://www.docker.com/
- Google Drive

- List all external resources, datasets, libraries, and tools you used (with links).
