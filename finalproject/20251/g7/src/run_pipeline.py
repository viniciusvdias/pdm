#!/usr/bin/env python3
"""
Script principal para execu√ß√£o do pipeline completo de big-data
An√°lise de Constrained-Off E√≥lico
"""

import os
import sys
import logging
from pathlib import Path
import argparse
from datetime import datetime

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline_execution.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def check_dependencies():
    """Verifica se todas as depend√™ncias est√£o instaladas"""
    logger.info("Verificando depend√™ncias...")
    
    required_packages = [
        'pandas', 'numpy', 'matplotlib', 'seaborn', 
        'plotly', 'streamlit', 'pyarrow', 'fastparquet'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            logger.info(f"‚úì {package} instalado")
        except ImportError:
            missing_packages.append(package)
            logger.error(f"‚úó {package} n√£o encontrado")
    
    if missing_packages:
        logger.error(f"Pacotes faltando: {', '.join(missing_packages)}")
        logger.error("Execute: pip install -r requirements.txt")
        return False
    
    logger.info("Todas as depend√™ncias est√£o instaladas!")
    return True

def check_consolidated_files():
    """Verifica se os arquivos consolidados existem"""
    logger.info("Verificando arquivos consolidados...")
    
    main_file = Path("restricoes_coff_eolicas_consolidado.csv")
    detail_file = Path("restricoes_coff_eolicas_detalhamento_consolidado.csv")
    
    if not main_file.exists():
        logger.error(f"Arquivo {main_file} n√£o encontrado!")
        logger.error("Execute primeiro: python transform.py")
        return False
    
    logger.info(f"‚úì Arquivo principal encontrado: {main_file}")
    
    if detail_file.exists():
        logger.info(f"‚úì Arquivo de detalhamento encontrado: {detail_file}")
    else:
        logger.warning(f"‚ö† Arquivo de detalhamento n√£o encontrado: {detail_file}")
        logger.info("O pipeline continuar√° apenas com os dados principais")
    
    return True

def run_step(step_name, step_function, *args, **kwargs):
    """Executa um passo do pipeline com tratamento de erro"""
    logger.info(f"=== EXECUTANDO: {step_name} ===")
    
    start_time = datetime.now()
    
    try:
        result = step_function(*args, **kwargs)
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info(f"‚úì {step_name} conclu√≠do com sucesso em {duration}")
        return True, result
        
    except Exception as e:
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.error(f"‚úó Erro em {step_name} ap√≥s {duration}: {str(e)}")
        return False, None

def step1_consolidation():
    """Passo 1: Verifica√ß√£o dos dados consolidados"""
    logger.info("Verificando se os dados j√° est√£o consolidados...")
    
    main_file = Path("restricoes_coff_eolicas_consolidado.csv")
    if main_file.exists():
        logger.info("‚úì Dados j√° consolidados encontrados!")
        return True
    else:
        logger.info("Executando consolida√ß√£o dos dados...")
        from transform import consolidate_files
        consolidate_files()
        return True

def step2_big_data_pipeline():
    """Passo 2: Pipeline de big-data"""
    from big_data_pipeline import WindFarmBigDataPipeline
    
    pipeline = WindFarmBigDataPipeline()
    pipeline.run_complete_pipeline()
    return True

def step3_anomaly_detection():
    """Passo 3: Detec√ß√£o de anomalias"""
    from anomaly_detection_analysis import AnomalyDetectionAnalysis
    
    analysis = AnomalyDetectionAnalysis()
    analysis.run_complete_analysis()
    return True

def step4_dashboard():
    """Passo 4: Dashboard (opcional)"""
    logger.info("Dashboard dispon√≠vel via: streamlit run visualization_dashboard.py")
    return True

def main():
    """Fun√ß√£o principal"""
    
    parser = argparse.ArgumentParser(description="Pipeline de Big-Data para An√°lise de Constrained-Off E√≥lico")
    parser.add_argument(
        "--steps", 
        nargs="+", 
        choices=["1", "2", "3", "4", "all"],
        default=["all"],
        help="Passos a executar (1=verifica√ß√£o, 2=pipeline, 3=anomalias, 4=dashboard, all=todos)"
    )
    parser.add_argument(
        "--skip-checks", 
        action="store_true",
        help="Pular verifica√ß√µes de depend√™ncias e dados"
    )
    
    args = parser.parse_args()
    
    logger.info("üöÄ Iniciando Pipeline de Big-Data - An√°lise de Constrained-Off E√≥lico")
    logger.info(f"Passos selecionados: {args.steps}")
    
    # Verifica√ß√µes iniciais
    if not args.skip_checks:
        if not check_dependencies():
            logger.error("Falha na verifica√ß√£o de depend√™ncias. Abortando.")
            return 1
        
        if not check_consolidated_files():
            logger.error("Falha na verifica√ß√£o de dados consolidados. Abortando.")
            return 1
    
    # Definir passos a executar
    if "all" in args.steps:
        steps_to_run = ["1", "2", "3", "4"]
    else:
        steps_to_run = args.steps
    
    # Executar passos
    step_functions = {
        "1": ("Verifica√ß√£o de Dados Consolidados", step1_consolidation),
        "2": ("Pipeline de Big-Data", step2_big_data_pipeline),
        "3": ("Detec√ß√£o de Anomalias", step3_anomaly_detection),
        "4": ("Dashboard", step4_dashboard)
    }
    
    successful_steps = []
    failed_steps = []
    
    for step_id in steps_to_run:
        if step_id in step_functions:
            step_name, step_func = step_functions[step_id]
            success, result = run_step(step_name, step_func)
            
            if success:
                successful_steps.append(step_id)
            else:
                failed_steps.append(step_id)
                logger.warning(f"Continuando com pr√≥ximos passos...")
        else:
            logger.warning(f"Passo {step_id} n√£o reconhecido, pulando...")
    
    # Resumo final
    logger.info("=" * 60)
    logger.info("üìä RESUMO DA EXECU√á√ÉO")
    logger.info("=" * 60)
    
    if successful_steps:
        logger.info(f"‚úÖ Passos executados com sucesso: {', '.join(successful_steps)}")
    
    if failed_steps:
        logger.error(f"‚ùå Passos com falha: {', '.join(failed_steps)}")
    
    if not failed_steps:
        logger.info("üéâ Pipeline executado com sucesso!")
        
        # Instru√ß√µes finais
        logger.info("\nüìã PR√ìXIMOS PASSOS:")
        logger.info("1. Verifique os arquivos gerados em processed_data/")
        logger.info("2. Execute o dashboard: streamlit run visualization_dashboard.py")
        logger.info("3. Consulte os relat√≥rios gerados")
        logger.info("4. Analise os logs em pipeline_execution.log")
        
        return 0
    else:
        logger.error("‚ö†Ô∏è Pipeline executado com falhas. Verifique os logs.")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 