{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40c4b8d-483c-4113-a577-0b9af67a2d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/09 18:35:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/09 18:35:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, when, udf, to_date, abs as abs_\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Music Data Cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Carregue seu arquivo parquet\n",
    "df = spark.read.parquet(\"/spark-data/meuarquivo.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117d4edd-748b-48ca-a4dc-ab543a7376fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número inicial de linhas: 551443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas após remover duplicatas: 498052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, split, substring\n",
    "\n",
    "# Vamos imprimir a contagem inicial de linhas para comparação\n",
    "initial_count = df.count()\n",
    "print(f\"Número inicial de linhas: {initial_count}\")\n",
    "\n",
    "# --- Passo 1: Limpar dados duplicados ---\n",
    "# Remove linhas onde a combinação de artista, álbum e música é idêntica.\n",
    "df_no_duplicates = df.dropDuplicates(['Artist(s)', 'Album', 'song'])\n",
    "print(f\"Linhas após remover duplicatas: {df_no_duplicates.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a5d010-f84f-4244-b819-8d7675834ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/python/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "                                                                                "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Passo 2: Tratar dados nulos ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Remove linhas onde colunas essenciais como 'song', 'Album' ou 'Time signature' são nulas.\u001b[39;00m\n\u001b[32m      3\u001b[39m df_no_nulls = df_no_duplicates.na.drop(subset=[\u001b[33m'\u001b[39m\u001b[33msong\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAlbum\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTime signature\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLinhas após remover nulos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_no_nulls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/dataframe.py:1240\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   1219\u001b[39m \n\u001b[32m   1220\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Passo 2: Tratar dados nulos ---\n",
    "# Remove linhas onde colunas essenciais como 'song', 'Album' ou 'Time signature' são nulas.\n",
    "df_no_nulls = df_no_duplicates.na.drop(subset=['song', 'Album', 'Time signature'])\n",
    "print(f\"Linhas após remover nulos: {df_no_nulls.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153f3c4-4b87-45f9-81e6-ebae76e676a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Passo 3: Limpar e transformar colunas ---\n",
    "df_transformed = df_no_nulls.withColumn(\n",
    "    # Deixar 'Release Date' apenas com o ano\n",
    "    \"release_year\",\n",
    "    substring(col(\"Release Date\"), -4, 4).cast(\"int\")\n",
    ").withColumn(\n",
    "    # Deixar 'Genre' apenas com o primeiro valor\n",
    "    \"main_genre\",\n",
    "    split(col(\"Genre\"), \",\")[0]\n",
    ").withColumn(\n",
    "    # Descaptalizar os valores da coluna 'emotion'\n",
    "    \"emotion_lower\",\n",
    "    lower(col(\"emotion\"))\n",
    ").withColumn(\n",
    "    # Loudness from string to float\n",
    "    \"Loudness (db)\",\n",
    "    abs_(regexp_replace(col(\"Loudness (db)\"), \"db\", \"\").cast(\"double\"))\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25bbd1-40ca-4036-931e-04078d4146c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Deixa as colunas em formato numerico\n",
    "numeric_cols = [\n",
    "    \"Tempo\", \"Energy\", \"Danceability\", \"Positiveness\",\n",
    "    \"Speechiness\", \"Liveness\", \"Acousticness\", \"Instrumentalness\", \"Popularity\"\n",
    "]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    df_transformed = df_transformed.withColumn(c, col(c).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7f870-c2b1-4ddd-9694-b4bb9b929f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Deixar apenas uma forma de escrita de hip-hop na coluna genero ---\n",
    "df_transformed = df_transformed.withColumn(\"main_genre\", \n",
    "    when(df_transformed[\"main_genre\"] == \"hip hop\", \"hip-hop\")\n",
    "    .otherwise(df_transformed[\"main_genre\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d42f80-6166-4b4d-8b9a-1376f6c32f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Passo 4: Apagar emoções com poucos valores ---\n",
    "# Lista de emoções a serem removidas (já em minúsculas)\n",
    "emotions_to_remove = ['true', 'pink', 'thirst', 'angry', 'confusion', 'interest']\n",
    "\n",
    "df_filtered = df_transformed.filter(\n",
    "    ~col(\"emotion_lower\").isin(emotions_to_remove)\n",
    ")\n",
    "print(f\"Linhas após filtrar emoções: {df_filtered.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb05b9-0b32-48f8-a61b-9df28c0ec092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Passo 5: Apagar colunas que não vamos usar ---\n",
    "columns_to_drop = [\n",
    "    # Colunas originais que foram transformadas\n",
    "    'Release Date', 'Genre', 'emotion',\n",
    "    # Colunas que o usuário pediu para apagar\n",
    "    'song',\n",
    "    'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1',\n",
    "    'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2',\n",
    "    'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3'\n",
    "]\n",
    "\n",
    "df_final = df_filtered.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4577e-7b5d-4cf1-a0e7-46ac898f7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Passo 5: Apagar colunas que não vamos usar ---\n",
    "columns_to_drop = [\n",
    "    # Colunas originais que foram transformadas\n",
    "    'Release Date', 'Genre', 'emotion',\n",
    "    # Colunas que o usuário pediu para apagar\n",
    "    'song',\n",
    "    'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1',\n",
    "    'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2',\n",
    "    'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3'\n",
    "]\n",
    "\n",
    "df_final = df_filtered.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# --- Resultado Final ---\n",
    "print(\"\\nEsquema do DataFrame final:\")\n",
    "df_final.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados limpos e pré-processados:\")\n",
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d29310-1ab2-4c30-a553-236f30102430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que seu DataFrame final e limpo se chama 'df_final'\n",
    "\n",
    "# 1. Defina o caminho de saída para o novo dataset\n",
    "caminho_saida = \"/spark-data/musicas_limpas_cluster.parquet\"\n",
    "\n",
    "# 2. Salve o DataFrame no formato Parquet\n",
    "df_final.write.mode(\"overwrite\").parquet(caminho_saida)\n",
    "\n",
    "print(f\"DataFrame limpo foi salvo com sucesso em: {caminho_saida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eedcff-e409-4290-8e91-32bf080c955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedbdaf-ac52-40a2-a507-7e7028b37bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
