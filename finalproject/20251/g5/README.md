# Relat√≥rio Final do Projeto: *Processamento de Dados Musicais do Spotify com PySpark*
## 1. Contexto e motiva√ß√£o

Nos √∫ltimos anos, com o crescimento de plataformas de streaming como o Spotify, uma enorme quantidade de dados musicais passou a ser gerada, incluindo letras, metadados, m√©tricas de √°udio e classifica√ß√µes emocionais. Esse volume de dados abre oportunidades para an√°lises em larga escala sobre padr√µes lingu√≠sticos, sentimentos e rela√ß√µes entre linguagem e m√∫sica.

O objetivo principal deste projeto √© analisar e processar letras de m√∫sicas em escala utilizando ferramentas de Big Data, em particular Apache Spark, com foco em:

- Identificar as palavras mais associadas a diferentes emo√ß√µes musicais

- Avaliar a distribui√ß√£o e frequ√™ncia dessas palavras em um corpus massivo

- Otimizar o processamento atrav√©s de formato Parquet e execu√ß√£o distribu√≠da

O projeto utiliza um dataset real com aproximadamente 500 mil m√∫sicas (extra√≠do do Spotify), com atributos como g√™nero, artista, letra e emo√ß√£o associada, o que seria invi√°vel de processar eficientemente com ferramentas tradicionais.

Al√©m disso, o trabalho prop√µe uma infraestrutura distribu√≠da replic√°vel via Docker Swarm, simulando um cluster Spark com m√∫ltiplos n√≥s, de forma pr√°tica e port√°til, permitindo testar desempenho, escalabilidade e custo computacional em diferentes configura√ß√µes.

## 2. Dados üéß

Este projeto utiliza o dataset ‚Äú500K+ Spotify Songs with Lyrics,Emotions & More‚Äù, dispon√≠vel publicamente no Kaggle:

üîó Fonte oficial: https://www.kaggle.com/datasets/devdope/900k-spotify

### 2.1 **O que o dataset cont√©m**:
- Aproximadamente 500 mil registros de m√∫sicas
- Cada linha representa uma m√∫sica e seus dados
- Tamanho aproximado: 1,3 GB
- Est√° dispon√≠vel nos formatos .csv e .json
- S√£o 39 colunas com atributos como:
    - Nome da m√∫sica, artista, √°lbum, g√™nero, data de lan√ßamento
    - Letra completa da m√∫sica
    - Emo√ß√£o da m√∫sica (campo emotion) associada √† letra
    - Caracter√≠sticas T√©cnicas e popularidade da m√∫sica
    - Indicadores de Atmosfera Musical (energia, positividade, dan√ßa, ac√∫stica, instrumentalidade etc.)
    - Usos Sugeridos para a Faixa (Good for Party, Good for Work, etc)
    - Indica√ß√µes por Similaridade Sonora

### 2.2 Como obter os dados


## 3. Como rodar o projeto

1. Clone o reposit√≥rio do projeto
2. Acesse a branch do grupo e a a pasta do projeto do grupo
```bash
git checkout finalproject-20251-G5

cd finalproject/20251/g5
```
3. Execute o script de configura√ß√£o inicial e ative as configura√ß√µes no terminal
```bash
./bin/pdmtf setup
source ~/.bashrc
pdmtf
```
### ‚öôÔ∏è **Inicializa√ß√£o do ambiente com Docker Swarm**

1. Acesse a pasta `src` do projeto para rodar os comandos `pdmtf`
```bash
cd src
```
2. Inicie os servi√ßos Spark com Jupyter usando Docker Swarm, esse comando sobe os servi√ßos: Jupyter Notebook, Spark Master e Spark Worker
```bash
pdmtf init
```
3. Execute um `docker ps` para verificar se o containers est√£o ativos e acesse a interface web em `http://localhost:8888/lab?token=spark123`

4. Execute `pdmtf help` para visualizar outros comandos dispon√≠veis como `pdmtf stop` (Parar o cluster Spark) ou `pdmtf scale 3` (Alterar o n√∫mero de workers)


## 4. Arquitetura do projeto

O projeto utiliza cont√™ineres orquestrados com Docker Swarm para processar dados com Apache Spark e interagir via Jupyter Notebook. Seus principais componentes s√£o:

- **Usu√°rio (navegador web):** Acessa a interface do Jupyter Notebook em `http://localhost:8888` para escrever e executar scripts PySpark. O navegador n√£o integra o cluster, mas √© o ponto de entrada para o usu√°rio.

- **Jupyter Notebook (Spark Driver):** Executado em um cont√™iner, fornece a interface web e atua como Spark Driver. Inicia a SparkSession e envia os jobs para o cluster via `spark://spark-master:7077`. Compartilha o volume /spark-data com os demais servi√ßos.

- **Spark Master:** Coordena o cluster, recebendo jobs do Driver e distribuindo-os entre os Workers. Roda em cont√™iner pr√≥prio e exp√µe sua interface de monitoramento em `http://localhost:8080`.

- **Spark Workers:** Executam as tarefas de forma distribu√≠da. Conectam-se automaticamente ao Master e acessam o volume /spark-data para leitura e escrita. S√£o cont√™ineres escal√°veis no Docker Swarm.

- **Volume /spark-data:** Pasta compartilhada entre todos os cont√™ineres, usada para armazenar dados de entrada (.csv) e sa√≠da (.parquet). Elimina a necessidade de transfer√™ncia via rede.

- **Rede spark-net:** Interliga todos os cont√™ineres, permitindo comunica√ß√£o entre os servi√ßos.

<p align="center">
  <img src="presentation/arquitetura-big.png" alt="Texto alternativo" width="700"/>
</p>

- Fluxo de Dados:

  ```
  [CSV] ‚Üí [Transforma√ß√£o Parquet] ‚Üí [Limpeza] ‚Üí [Processamento] ‚Üí [Resultados]
  ```



## 5. Workloads evaluated

#### [WORKLOAD-1] Agrupamento de letras de m√∫sicas por emo√ß√£o

**Objetivo:** Mostrar quais palavras s√£o mais frequentes nas letras das m√∫sicas em cada emo√ß√£o.

**Etapas:**
- Leitura do dataset no formato Parquet.
- Remo√ß√£o de pontua√ß√£o e s√≠mbolos usando `regex_replace`.
- Remo√ß√£o de StopWords (palavras sem sentido sem√¢ntico).
- Tokeniza√ß√£o por espa√ßos.
- Convers√£o para min√∫sculas.
- Explos√£o das palavras em linhas individuais usando `explode`.
- Ranqueamento de palavras mais recorrentes por emo√ß√£o.

---

#### [WORKLOAD-2] C√°lculo da Similaridade de Jaccard

**Objetivo:** Comparar todos os pares poss√≠veis de artistas dentro do mesmo g√™nero utilizando a m√©trica de Jaccard com base no vocabul√°rio textual.

**Etapas:**
- Realiza√ß√£o de um self-join (cruzamento) entre os artistas por g√™nero.
- C√°lculo de interse√ß√£o (`array_intersect`) e uni√£o (`array_union`) de vocabul√°rios.
- C√°lculo da similaridade de Jaccard: `interse√ß√£o / uni√£o`.
- Ranqueamento dos pares mais similares por g√™nero.

---

#### [WORKLOAD-3] Similaridade L√©xica entre G√™neros Musicais (Sem Numpy)

**Objetivo:** Encontrar o n√∫mero de palavras em comum entre os principais g√™neros musicais, medindo similaridade l√©xica sem o uso de bibliotecas externas como NumPy.

**Etapas**:

- Leitura do dataset `musicas_limpas.parquet`.
- Sele√ß√£o dos 10 g√™neros musicais com mais m√∫sicas no dataset.
- Explos√£o das palavras com `explode` e `split`.
- Agrupamento por `main_genre` usando `collect_set("palavra")` para obter vocabul√°rio √∫nico por g√™nero.
- Self-join do vocabul√°rio de g√™neros usando `crossJoin`, comparando pares distintos (`g1 < g2`).
- C√°lculo do n√∫mero de palavras em comum por par de g√™neros usando `array_intersect` + `size`.
- Ordena√ß√£o decrescente pelo n√∫mero de palavras em comum.

---

- Specify each big data task evaluated in your project (queries, data pre-processing, sub-routines, etc.).
- Be specific: describe the details of each workload, and give each a clear name. These named workloads will be referenced and evaluated via performance experiments in the next section.
  - Example: [WORKLOAD-1] Query that computes the average occupation within each
    time window (include query below). [WORKLOAD-2] Preprocessing, including
  removing duplicates, standardization, etc.

---

## 6. Experiments and results

### 6.1 Experimental environment

- As execu√ß√µes foram realizadas em ambiente Docker com Spark.
- Os experimentos foram realizados em uma m√°quina:
> Windows 11
> Ubuntu 22.04 (containers)
> Docker Version 28.3.0
> Spark Version 3.4.1
> Modo Cluster
> 1 Spark Master + N workers (var√≠avel)
> CPU 3.70 Ghz base (at√© 4.6 Ghz)
> 16 GB de RAM DDR4 3200 MHz

### 6.2 What did you test?

Foram avaliadas as seguintes varia√ß√µes de configura√ß√£o:

| Par√¢metro                 | Valores Testados              |
|--------------------------|-------------------------------|
| Workers                  | 1, 2, 6                        |
| N√∫cleos por Worker       | 1, 2, 6                        |
| M√©tricas observadas      | Tempo total (s), uso de mem√≥ria, distribui√ß√£o de tarefas, uso de CPU |
| Repeti√ß√µes               | 3 execu√ß√µes por configura√ß√£o   |

### 6.3 Results
#### Tabela Comparativa por Configura√ß√£o


## 7. Discussion and conclusions

- Summarize what worked and what did not.
- Discuss any challenges or limitations of this work.

## 8. References and external resources

- List all external resources, datasets, libraries, and tools you used (with links).