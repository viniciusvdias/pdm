{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739c9474-8952-4e55-947f-3799a475d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a271ab-3aa5-4a65-ae27-88ccfae4ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONSTANTES DE COLUNAS\n",
    "COL_DATA = \"Data\"\n",
    "COL_TEMP_MAX = \"TEMPERATURA MÁXIMA NA HORA ANT. (AUT) (°C)\"\n",
    "COL_TEMP_MIN = \"TEMPERATURA MÍNIMA NA HORA ANT. (AUT) (°C)\"\n",
    "COL_PREC = \"PRECIPITAÇÃO TOTAL, HORÁRIO (mm)\"\n",
    "COL_STATE = \"state\"\n",
    "COL_REGION = \"regiao\"\n",
    "\n",
    "# --- MAPA DE ARQUIVOS POR REGIÃO\n",
    "FILES = {\n",
    "    \"norte\":        \"dataset/north.csv\",\n",
    "    \"nordeste\":    \"dataset/northeast.csv\",\n",
    "    \"centro-oeste\": \"dataset/central_west.csv\",\n",
    "    \"sudeste\":    \"dataset/southeast.csv\",\n",
    "    \"sul\":        \"dataset/south.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec396634-0087-4312-aa21-64330cad2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat(files_map):\n",
    "    dfs = []\n",
    "    for reg, path in files_map.items():\n",
    "        df = dd.read_csv(\n",
    "            path,\n",
    "            assume_missing=True,\n",
    "            parse_dates=[COL_DATA],\n",
    "            low_memory=False\n",
    "        )\n",
    "        df = df.assign(regiao=reg)\n",
    "        dfs.append(df)\n",
    "    return dd.concat(dfs)\n",
    "\n",
    "def clean_data(df, cols_to_clean, state_col=None):\n",
    "    for col in cols_to_clean:\n",
    "        df = df.astype({col: \"float64\"})\n",
    "    df = df.dropna(subset=cols_to_clean)\n",
    "    cond = True\n",
    "    for col in cols_to_clean:\n",
    "        cond = cond & (df[col] != -9999)\n",
    "    df = df[cond]\n",
    "    if state_col:\n",
    "        df = df.dropna(subset=[state_col])\n",
    "    return df\n",
    "\n",
    "def add_time_columns(df):\n",
    "    df[\"ano\"] = df[COL_DATA].dt.year\n",
    "    df[\"mes\"] = df[COL_DATA].dt.month\n",
    "    return df\n",
    "\n",
    "def get_estacao(mes):\n",
    "    if mes in [12, 1, 2]:\n",
    "        return \"Verão\"\n",
    "    elif mes in [3, 4, 5]:\n",
    "        return \"Outono\"\n",
    "    elif mes in [6, 7, 8]:\n",
    "        return \"Inverno\"\n",
    "    else:\n",
    "        return \"Primavera\"\n",
    "\n",
    "def add_estacao_column(df):\n",
    "    df[\"estacao\"] = df[\"mes\"].map(get_estacao)\n",
    "    return df\n",
    "\n",
    "def aggregate_annual_precipitation_by_region(df):\n",
    "    agg = df.groupby([\"regiao\", \"ano\"])[COL_PREC].sum().rename(\"precipitacao_total_anual\")\n",
    "    return agg.compute().reset_index()\n",
    "\n",
    "def aggregate_annual_temperature_by_region(df):\n",
    "    agg = df.groupby([\"regiao\", \"ano\"])[[COL_TEMP_MAX]].agg({\n",
    "        COL_TEMP_MAX: \"mean\"\n",
    "    }).rename(columns={\n",
    "        COL_TEMP_MAX: \"temp_media\"\n",
    "    })\n",
    "    return agg.compute().reset_index()\n",
    "\n",
    "def aggregate_by_state(df):\n",
    "    agg = df.groupby(COL_STATE)[\n",
    "        [COL_TEMP_MAX, COL_TEMP_MIN, COL_PREC]\n",
    "    ].agg({\n",
    "        COL_TEMP_MAX: \"mean\",\n",
    "        COL_TEMP_MIN: \"mean\",\n",
    "        COL_PREC: \"sum\"\n",
    "    }).rename(columns={\n",
    "        COL_TEMP_MAX: \"temp_max_media\",\n",
    "        COL_TEMP_MIN: \"temp_min_media\",\n",
    "        COL_PREC: \"precipitacao_total\"\n",
    "    })\n",
    "    return agg.compute().reset_index()\n",
    "\n",
    "def aggregate_by_season(df):\n",
    "    agg = df.groupby(\"estacao\")[[COL_TEMP_MAX, COL_TEMP_MIN, COL_PREC]].agg({\n",
    "        COL_TEMP_MAX: \"mean\",\n",
    "        COL_TEMP_MIN: \"mean\",\n",
    "        COL_PREC: \"sum\"\n",
    "    }).rename(columns={\n",
    "        COL_TEMP_MAX: \"temp_max_media\",\n",
    "        COL_TEMP_MIN: \"temp_min_media\",\n",
    "        COL_PREC: \"precipitacao_total\"\n",
    "    })\n",
    "    return agg.compute().reset_index()\n",
    "\n",
    "def plot_bar(df, x, y_vars, title=None, width=1200, height=600):\n",
    "    df_melted = df.melt(id_vars=[x], value_vars=y_vars)\n",
    "    fig = px.bar(\n",
    "        df_melted,\n",
    "        x=x,\n",
    "        y=\"value\",\n",
    "        color=\"variable\",\n",
    "        barmode=\"group\",\n",
    "        labels={\n",
    "            x: x.capitalize(),\n",
    "            \"value\": \"Valor\",\n",
    "            \"variable\": \"Métrica\"\n",
    "        },\n",
    "        title=title\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        bargap=0.2,\n",
    "        legend_title=\"Métrica\"\n",
    "    )\n",
    "    fig.write_html(f\"../misc/{(title.replace(',', '').lower()).replace(' ', '_')}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95200e5d-5243-485e-b5c6-9e52c34613c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/jovyan/work/src/dataset/north.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/dataframe/io/csv.py:731\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    719\u001b[0m     urlpath,\n\u001b[1;32m    720\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    730\u001b[0m ):\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/dataframe/io/csv.py:524\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m     sample \u001b[38;5;241m=\u001b[39m blocksize\n\u001b[0;32m--> 524\u001b[0m b_out \u001b[38;5;241m=\u001b[39m \u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_lineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_path_column:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/bytes/core.py:111\u001b[0m, in \u001b[0;36mread_bytes\u001b[0;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, include_path, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot do chunked reads on compressed files. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo read, set blocksize=None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[0;32m--> 111\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:100\u001b[0m, in \u001b[0;36mLocalFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[0;32m--> 100\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m link \u001b[38;5;241m=\u001b[39m stat\u001b[38;5;241m.\u001b[39mS_ISLNK(out\u001b[38;5;241m.\u001b[39mst_mode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/work/src/dataset/north.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m clean_data(df, [COL_TEMP_MAX, COL_TEMP_MIN, COL_PREC], state_col\u001b[38;5;241m=\u001b[39mCOL_STATE)\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m add_time_columns(df)\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mload_and_concat\u001b[0;34m(files_map)\u001b[0m\n\u001b[1;32m      2\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reg, path \u001b[38;5;129;01min\u001b[39;00m files_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mCOL_DATA\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39massign(regiao\u001b[38;5;241m=\u001b[39mreg)\n\u001b[1;32m     11\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/backends.py:151\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/jovyan/work/src/dataset/north.csv'"
     ]
    }
   ],
   "source": [
    "df = load_and_concat(FILES)\n",
    "df = clean_data(df, [COL_TEMP_MAX, COL_TEMP_MIN, COL_PREC], state_col=COL_STATE)\n",
    "df = add_time_columns(df)\n",
    "df = add_estacao_column(df)\n",
    "\n",
    "# 1) Precipitação anual por região\n",
    "annual_precip = aggregate_annual_precipitation_by_region(df)\n",
    "fig = px.bar(\n",
    "    annual_precip,\n",
    "    x=\"ano\",\n",
    "    y=\"precipitacao_total_anual\",\n",
    "    color=\"regiao\",\n",
    "    barmode=\"group\",\n",
    "    labels={\n",
    "        \"ano\": \"Ano\",\n",
    "        \"precipitacao_total_anual\": \"Precipitação Total Anual (mm)\",\n",
    "        \"regiao\": \"Região\"\n",
    "    },\n",
    "    title=\"Precipitação Total Anual por Região\"\n",
    ")\n",
    "fig.update_layout(width=1200, height=500, bargap=0.2, legend_title=\"Região\")\n",
    "fig.write_html(\"../misc/grafico_precipitacao_anual_por_regiao.html\")\n",
    "\n",
    "# 2) Temperatura média anual por região\n",
    "annual_temp = aggregate_annual_temperature_by_region(df)\n",
    "fig = px.bar(\n",
    "    annual_temp,\n",
    "    x=\"ano\",\n",
    "    y=\"temp_media\",\n",
    "    color=\"regiao\",\n",
    "    barmode=\"group\",\n",
    "    labels={\n",
    "        \"ano\": \"Ano\",\n",
    "        \"temp_media\": \"Temperatura Média (°C)\",\n",
    "        \"regiao\": \"Região\"\n",
    "    },\n",
    "    title=\"Temperatura Média Anual por Região\"\n",
    ")\n",
    "fig.update_layout(width=1200, height=500, bargap=0.2, legend_title=\"Região\")\n",
    "fig.write_html(\"../misc/grafico_temperatura_media_anual_por_regiao.html\")\n",
    "\n",
    "# 3) Temperatura média e precipitação por estado\n",
    "by_state = aggregate_by_state(df)\n",
    "plot_bar(\n",
    "    by_state,\n",
    "    x=\"state\",\n",
    "    y_vars=[\"temp_max_media\", \"temp_min_media\", \"precipitacao_total\"],\n",
    "    title=\"Média Temperaturas e Precipitação Total por Estado (Todos os Anos)\"\n",
    ")\n",
    "\n",
    "# 4) Temperatura e precipitação por estação do ano\n",
    "by_season = aggregate_by_season(df)\n",
    "plot_bar(\n",
    "    by_season,\n",
    "    x=\"estacao\",\n",
    "    y_vars=[\"temp_max_media\", \"temp_min_media\", \"precipitacao_total\"],\n",
    "    title=\"Temperaturas e Precipitação por Estação do Ano (Todos os Anos)\",\n",
    "    width=1100,\n",
    "    height=550\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
